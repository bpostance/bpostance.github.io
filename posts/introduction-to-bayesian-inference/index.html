<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Bayesian Inference by hand" /><meta name="author" content="Ben Postance" /><meta property="og:locale" content="en_US" /><meta name="description" content="image source" /><meta property="og:description" content="image source" /><link rel="canonical" href="https://bpostance.github.io/posts/introduction-to-bayesian-inference/" /><meta property="og:url" content="https://bpostance.github.io/posts/introduction-to-bayesian-inference/" /><meta property="og:site_name" content="Ben Postance" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-12-13T18:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Bayesian Inference by hand" /><meta name="twitter:site" content="@benpostance" /><meta name="twitter:creator" content="@Ben Postance" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Postance"},"dateModified":"2021-04-19T20:43:27+01:00","datePublished":"2020-12-13T18:00:00+00:00","description":"image source","headline":"Bayesian Inference by hand","mainEntityOfPage":{"@type":"WebPage","@id":"https://bpostance.github.io/posts/introduction-to-bayesian-inference/"},"url":"https://bpostance.github.io/posts/introduction-to-bayesian-inference/"}</script><title>Bayesian Inference by hand | Ben Postance</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> // see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> MathJax = { tex: { inlineMath: [ // start/end delimiter pairs for in-line math ['$','$'], ['\\(','\\)'] ], displayMath: [ // start/end delimiter pairs for display math ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-140894462-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-140894462-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="top"></div><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/7165735?s=460&u=7ca34b8152bbff087bfc860cc87a4c35b4f9cb1b&v=4" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Ben Postance</a></div><div class="site-subtitle font-italic">Risk, Data Science and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/bpostance" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/benpostance" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['benpostance','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Bayesian Inference by hand</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Bayesian Inference by hand</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Ben Postance </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Dec 13, 2020, 6:00 PM +0000" prep="on" > Dec 13, 2020 <i class="unloaded">2020-12-13T18:00:00+00:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Apr 19, 2021, 8:43 PM +0100" prefix="Updated " > Apr 19, 2021 <i class="unloaded">2021-04-19T20:43:27+01:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1948 words">10 min</span></div></div><div class="post-content"><p> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://i.stack.imgur.com/07YsT.png" alt="drawing" width="800" height="300" class="center" /></p><p><a href="https://stats.stackexchange.com/q/86472/100439"><sub><em>image source</em></sub></a></p><p><strong><a href="https://github.com/bpostance/dsc.learn/blob/main/statistics/bayesian-inference/01.0.bayesian-inference-byhand-binomial-Py.ipynb">Jupyter notebook here</a></strong></p><p>Bayesian inference is a statistical method used to update one’s beliefs about a process or system upon observing data. It has wide reaching applications from optimizing prices to developing probabilistic weather forecasting and risk models. In this post I will manually walk through the steps to perform Bayesian Inference. First, lets recap on Bayes’ theorem.</p>\[P(A|B) = \frac {P(B|A)P(A)}{P(B)}\]<p>In probability theory and statistics, Bayes’ theorem (alternatively Bayes’ law or Bayes’ rule), named after Reverend Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. <a href="https://plato.stanford.edu/archives/spr2019/entries/bayes-theorem/">1</a></p><p>I’m a visual learner and my favorite illustration to explain Bayes Theorem is this one <a href="https://www.countbayesie.com/blog/2015/2/18/bayes-theorem-with-lego">using lego bricks</a>:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2020-12-13-introduction-to-bayesian-inference-01.jpeg" width="400" height="400" /></p><p>The image shows a 60 lego unit area (the legosphere) with:</p><ul><li>40 blue areas<li>20 red areas<li>6 overlain yellow areas.</ul><p>Using this example can work through the maths of Bayes’ Theorem to determine the probabilities and conditional probabilities of each colour.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">pBlue</span> <span class="o">=</span> <span class="mi">40</span><span class="o">/</span><span class="mi">60</span>
<span class="n">pRed</span> <span class="o">=</span> <span class="mi">20</span><span class="o">/</span><span class="mi">60</span>
<span class="n">pYellow</span> <span class="o">=</span> <span class="mi">6</span><span class="o">/</span><span class="mi">60</span>
<span class="n">pYellowRed</span> <span class="o">=</span> <span class="mi">4</span><span class="o">/</span><span class="mi">20</span> <span class="c1"># probability of Yellow given Red
</span><span class="n">pYellowBlue</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">40</span> <span class="c1"># probability of Yellow given Blue
</span></pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Cond p(Yellow|Red) = 0.200
Cond p(Yellow|Blue) = 0.050
</pre></table></code></div></div><p>We now have the baseline information for the probability and conditional probability of landing on each colour within the Legosphere. We can apply Bayes’ theorem to generate estimates for “if we land on a yellow brick, what is the probability its red underneath?”</p>\[P(A|B) = \frac {P(B|A)P(A)}{P(B)}\] \[P(Red|Yellow) = \frac {P(Yellow|Red)P(Red)}{P(Yellow)}\]<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">pRedYellow</span> <span class="o">=</span> <span class="n">pYellowRed</span><span class="o">*</span><span class="n">pRed</span><span class="o">/</span><span class="n">pYellow</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Cond p(Red|Yellow) = {:.3f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">pRedYellow</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Cond p(Red|Yellow) = 0.667
</pre></table></code></div></div><p>You may now be thinking, “wait so Bayes’ rule is just the number of total number of Yellow pegs (6) over the number of yellow pegs on red squares (4)?”. Well yes in this case it is, and as you can see follows intuition and logic given that we know everything there is to know about the legosphere. However, imagine now that the lego brick image above is just one sample of a much larger lego board. We can apply Bayes’ rule to infer and update our beliefs (probability estimates) about the board as more samples are taken. This is called Bayesian Inference.</p><h2 id="bayesian-inference">Bayesian Inference</h2><p>The approach follows:</p><ol><li>Set prior assumptions and establish “known knowns” of our data based on heuristics, historical or sample data.<li>Formalize a Mathematical Model of the problem space and prior assumptions.<li>Formalize the Prior Distributions.<li>Apply Bayes’ theorem to derive the posterior parameter values from observed sample data.<li>Repeat steps 1-4 as more data samples are obtained.</ol><p>To illustrate these steps we will use a simple coin-toss experiment to determine wether our coin is bias or not.</p><h4 id="1-prior-assumptions">1. Prior Assumptions</h4><p>Here we will establish some assumptions and heuristic rules for our coin. We have a reasonable assumption that our coin is fair. The prior probability of landing a tails is 0.5. However, we also some observational sample data based on 50 tosses of the coin. It looks as though fewer tails were observed than might have been expected.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>Trials:	50
tails:	15
heads:	35
Observed P(tails) = 0.3
</pre></table></code></div></div><h4 id="2-the-mathematical-model">2. The Mathematical Model</h4><p>Now we build a mathematical model to represent the processes in our coin-tossing example. This model is devised to best capture the real-world physical processes, by means of their probability distribution parameters, that generate the observational data (i.e. the generative process of Heads and Tails). Naturally in many real-world applications such as weather forecasting these models can become quite complex as the number of processes that drive the data, and therefore number of probability distributions in the model, is quite large. In our coin toss example we have a relatively simple world where given the probability of landing a head as theta θ:</p>\[P(Y=1|θ)=θ\]<p>the probability of landing a tails is then:</p>\[P(Y=0|θ)=1-θ\]<p>Now select an appropriate probability distribution for each data or process in the model (e.g. gaussian, log-normal, gamma, poisson etc). You may recall that a coin-toss is a single Bernoulli trial where each trial results in either: Success or Failure, Heads or Tails, Conversion or non-conversion, Survive or Die, you get the picture. The <a href="https://www.statisticshowto.com/probability-and-statistics/binomial-theorem/binomial-distribution-formula/">binomial distribution</a> is the probability mass function of multiple independent <a href="https://www.unf.edu/~cwinton/html/cop4300/s09/class.notes/DiscreteDist.pdf">bernoulli trials</a>. Thus the binomial distribution describes the output of our coin toss observation data.</p><p>To recap then. Our mathematical model is configured to determine the most likely value for our coin to produce a tails, theta, given the sample of observed data. Recalling Bayes’ theorem this is:</p>\[P(θ|Data) = \frac {P(Data|θ)P(θ)}{P(Data)}\]<p>where P(θ|Data) is the posterior probability of theta given the data . It is our</p><ul><li>the conditional probability of theta given the data P(Data|θ) multiplied by<li>the prior distribution of theta P(θ) divided by<li>the probability of the data P(Data)</ul><p>Wait, wait, wait…you might now be wondering what a <a href="https://en.wikipedia.org/wiki/Conditional_independence">conditional probability</a> is and how on earth do I get the likelihood of the data?</p><p>The conditional probability in our case is simply the relationship between the occurrence of two phenomena, namely landing a coin on Tails and the observed data. It is identical to the red and yellow lego bricks we saw above. And, if we had more variables in our coin toss model (e.g. red, blue, yellow lego) then conditional probability simply asserts that the probability of first order events (e.g. red and blue squares) may be dependent on the probability of second order events (e.g. yellow), but that the first order events do not influence each other. See also <a href="https://math.stackexchange.com/questions/23093/could-someone-explain-conditional-independence">this intuitive conditional probability example</a> on Maths exchange.</p><p>The likelihood of the data P(Data) then is the “<em>function that describes a hyper-surface whose peak, if it exists, represents the combination of model parameter values that maximize the probability of drawing the sample obtained</em>” <sup><a href="https://en.wikipedia.org/wiki/Likelihood_function#Example">wikipedia</a></sup>. For discrete bernoulli trials again this is the binomial distribution as shown in the figure below.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2020-12-13-introduction-to-bayesian-inference-02.png" alt="png" /></p><h4 id="3-the-prior-distribution">3. The Prior Distribution</h4><p>Our assumption and prior belief is that the coin is fair. The probability of landing a tails P(Tails|θ)=0.5 and follows a binomial distribution. But in the bayesian world we treat our assumptions as point estimates within possible range of values. So rather than using our single 0.5 point estimate, we represent θ itself as a prior probability distribution with varying levels of uncertainty.</p><p>Now, in practice we can choose any distribution we like here so long as its integral is equal to 1. There are also whats called “<a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate-priors</a>” where the selected prior distribution is in the same family as the posterior distribution and that naturally makes the maths and model interpretation much cleaner. But conceptually they key consideration for us here is to ensure that the “shape” of the prior distribution reflects our prior beliefs and or uncertainties for the data.</p><p>Two applicable distributions are the <a href="https://en.wikipedia.org/wiki/Beta_distribution">beta</a> and <a href="https://en.wikipedia.org/wiki/Triangular_distribution">triangle</a> distributions. The beta distribution is commonly used to model percentages and probabilities, for instance in our case a beta-binomial distribution may be used and is in-fact a conjugate prior. But we could also use the triangle distribution. This is common in business applications and problems where observational data is scarce such that probability distributions are based on expert judgement such as in risk management.</p><p>Below we see the triangle and beta-binomial distributions to generate some estimates about our prior. The blue line represents the continuous beta-binomial priors, whereas the black stick-points are the triangle priors. In this case they are very similar.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="c1"># prior estimates
</span><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">triang</span><span class="p">,</span> <span class="n">beta</span>

<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.1</span><span class="p">,.</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># triangle priors
</span><span class="n">priors_t</span> <span class="o">=</span> <span class="n">triang</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>
<span class="n">priors_t</span> <span class="o">=</span> <span class="n">priors_t</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">priors_t</span><span class="p">)</span>
<span class="n">prior_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">:</span><span class="n">theta</span><span class="p">,</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">:</span><span class="n">priors_t</span><span class="p">})</span>

<span class="c1"># beta priors
</span><span class="n">priors_bb</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,)</span>
<span class="n">priors_bb</span> <span class="o">=</span> <span class="n">priors_bb</span><span class="o">/</span><span class="nf">sum</span><span class="p">(</span><span class="n">priors_bb</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2020-12-13-introduction-to-bayesian-inference-03.png" alt="png" /></p><h4 id="4-apply-bayes-theorem-on-observation-data">4. Apply Bayes Theorem on observation data</h4><p>We can now put this altogether to produce a posterior probability estimate.</p><p>To illustrate the bayesian inference process lets first update our priors using just one toss of the coin.</p><p>We toss our coin once and observe a tails.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Trials: 1
Result: T=1 H=0
</pre></table></code></div></div><p>Calling back to our model formula:</p>\[P(θ|Data) = \frac {P(Data|θ)P(θ)}{P(Data)}\]<ol><li>P(Data|θ) is given by the likelihood-function or equivalent binomial pmf for the sample.<li>P(θ) is the prior from our triangle or beta-binomial distribution.<li>P(Data) is the marginal likelihood or Bayes’ “model evidence” of the data from the integrated product of the prior and likelihood. Beware that this may be termed differently depending on context (<a href="https://en.wikipedia.org/wiki/Marginal_likelihood">see here</a>).</ol><p><strong>Results from the first sample</strong><br /> Below we see a table and plots showing the prior, likelihood and binomial , and posterior distributions for our first single coin toss observation.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">({</span><span class="sh">'</span><span class="s">tails</span><span class="sh">'</span><span class="p">:</span><span class="n">tails</span><span class="p">,</span>
                   <span class="sh">'</span><span class="s">heads</span><span class="sh">'</span><span class="p">:</span><span class="n">heads</span><span class="p">,</span>
                   <span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">:</span><span class="n">theta</span><span class="p">,</span>
                   <span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">:</span><span class="n">priors_bb</span><span class="p">,</span>
                   <span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">:[(</span><span class="n">x</span><span class="o">**</span><span class="n">tails</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="n">heads</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">theta</span><span class="p">],</span>
                   <span class="sh">'</span><span class="s">binom</span><span class="sh">'</span><span class="p">:</span><span class="n">binom</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">tails</span><span class="p">,</span><span class="n">trials</span><span class="p">,</span><span class="n">theta</span><span class="p">),</span>
                  <span class="p">})</span>

<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">marginal</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">likelihood</span><span class="sh">'</span><span class="p">]</span><span class="o">*</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">])</span> <span class="o">/</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">marginal</span><span class="sh">'</span><span class="p">]</span>
<span class="n">df</span>
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>tails<th>heads<th>theta<th>prior<th>likelihood<th>binom<th>marginal<th>posterior<tbody><tr><td>0<td>1<td>0<td>0.0<td>0.000000<td>0.0<td>0.0<td>0.5<td>0.000000<tr><td>1<td>1<td>0<td>0.1<td>0.004133<td>0.1<td>0.1<td>0.5<td>0.000827<tr><td>2<td>1<td>0<td>0.2<td>0.041287<td>0.2<td>0.2<td>0.5<td>0.016515<tr><td>3<td>1<td>0<td>0.3<td>0.122521<td>0.3<td>0.3<td>0.5<td>0.073512<tr><td>4<td>1<td>0<td>0.4<td>0.209015<td>0.4<td>0.4<td>0.5<td>0.167212<tr><td>5<td>1<td>0<td>0.5<td>0.246089<td>0.5<td>0.5<td>0.5<td>0.246089<tr><td>6<td>1<td>0<td>0.6<td>0.209015<td>0.6<td>0.6<td>0.5<td>0.250818<tr><td>7<td>1<td>0<td>0.7<td>0.122521<td>0.7<td>0.7<td>0.5<td>0.171529<tr><td>8<td>1<td>0<td>0.8<td>0.041287<td>0.8<td>0.8<td>0.5<td>0.066059<tr><td>9<td>1<td>0<td>0.9<td>0.004133<td>0.9<td>0.9<td>0.5<td>0.007440<tr><td>10<td>1<td>0<td>1.0<td>0.000000<td>1.0<td>1.0<td>0.5<td>0.000000</table></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2020-12-13-introduction-to-bayesian-inference-04.png" alt="png" /></p><p>Comparing the prior and posterior distributions shows very little difference, particularly in where theta is in the range of 0.4 to 0.6 where our prior estimate was “strongest”. Examining the posterior distribution we see a peak at 0.5, in line with our prior belief in the triangle distribution. Note also that the likelihood function does not really inform the model given that this sample, or “model evidence”, is just a single toss of the coin. In-fact the likelihood function is the straight linear line we saw above.</p><p>We can approximate <a href="https://en.wikipedia.org/wiki/Credible_interval#Choosing_a_credible_interval">credible intervals (CI)</a> to quantify the posterior distribution. For instance we see that:</p><ul><li>99 % CI that theta is between 0.2 and 0.8.<li>41 % CI that theta is between 0.4 and 0.6<li>P(θ|data) &gt; 0.4 is 91%. The integral of the area under curve.</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">c1</span><span class="p">,</span><span class="n">c2</span><span class="p">,</span><span class="n">p1</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mf">0.2</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">]</span><span class="o">&lt;=</span><span class="mf">0.8</span><span class="p">),</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">].</span><span class="nf">sum</span><span class="p">(),</span>
        <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mf">0.4</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">]</span><span class="o">&lt;=</span><span class="mf">0.6</span><span class="p">),</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">].</span><span class="nf">sum</span><span class="p">(),</span>
        <span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">theta</span><span class="sh">'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mf">0.4</span><span class="p">),</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span>
<span class="p">)</span>
</pre></table></code></div></div><p><strong>Results from a second sample</strong></p><p>Let’s now include all of our observation data (n=50) and update the posterior distribution once more.</p><p>Note that if we were pursuing true Bayesian Inference then we would in fact update our first posterior estimate with this new data. Thus the posterior from our first sample (n=1) estimate becomes our prior in the (n=50) sample and so on.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="c1"># observation data
</span><span class="n">trials</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tails</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">heads</span> <span class="o">=</span> <span class="n">trials</span><span class="o">-</span><span class="n">tails</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2020-12-13-introduction-to-bayesian-inference-05.png" alt="png" /></p><p>Now we clearly see that with our updated observations (n=50) our coin is very likely bias to heads. The likelihood and posterior distributions for a tails result peak around theta values of 0.3.</p><p>Again, this is confirmed by checking the Credible Intervals and integrals.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>CI 0.2-0.8 = 1.00
CI 0.4-0.6 = 0.36
P(theta|data)&gt;0.4 = 0.36
</pre></table></code></div></div><h3 id="conclusion">Conclusion</h3><p>In this post I have recapped on the basics of Bayes theorem and shown how to perform bayesian inference by-hand for the binomial distribution on a coin tossing example. In practice bayesian inference models are rarely built by hand. Instead we rely on statistical packages and frameworks that better handle probability distributions and sampling using monte-carlo methods. More on those in my next post.</p><h3 id="references">References</h3><p>I would like to credit <a href="https://tinyheero.github.io/2018/12/14/crux-of-bayes-statistics.html">Fong Chun Chan</a> for his useful materials on this subject.</p><ul><li>https://ro-che.info/articles/2016-06-14-predicting-coin-toss<li>https://www.vosesoftware.com/riskwiki/Bayesiananalysisexampleidentifyingaweightedcoin.php<li>https://www.analyticsvidhya.com/blog/2016/06/bayesian-statistics-beginners-simple-english/<li>https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348<li>https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/<li>https://www.psychologicalscience.org/observer/bayes-for-beginners-probability-and-likelihood<li>https://en.wikipedia.org/wiki/Credible_interval#Choosing_a_credible_interval<li>https://towardsdatascience.com/conditional-independence-the-backbone-of-bayesian-networks-85710f1b35b<li>https://math.stackexchange.com/questions/23093/could-someone-explain-conditional-independence</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog-post/'>blog-post</a>, <a href='/categories/data-analysis/'>data-analysis</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/bayesian-inference/" class="post-tag no-text-decoration" >bayesian-inference</a> <a href="/tags/bayes-theory/" class="post-tag no-text-decoration" >bayes-theory</a> <a href="/tags/monte-carlo/" class="post-tag no-text-decoration" >monte carlo</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Bayesian Inference by hand - Ben Postance&url=https://bpostance.github.io/posts/introduction-to-bayesian-inference/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Bayesian Inference by hand - Ben Postance&u=https://bpostance.github.io/posts/introduction-to-bayesian-inference/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Bayesian Inference by hand - Ben Postance&url=https://bpostance.github.io/posts/introduction-to-bayesian-inference/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-ai-enterprise/">How to build an Advanced Analytics function</a><li><a href="/posts/multi-tasking-in-python/">Multi-tasking in Python</a><li><a href="/posts/airflow-taskflow/">Writing Pythonic Airflow DAGs with the TaskFlow API</a><li><a href="/posts/finCEN/">The finCEN files: Uncovering money laundering patterns in the global banking network</a><li><a href="/posts/working-with-MODIS-data/">Geospatial Analysis: Working with MODIS data</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/bayes-theory/">bayes-theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian-inference</a> <a class="post-tag" href="/tags/data-mining/">data-mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data-cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension-reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial-analysis</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/bayesian-inference-pymc3/"><div class="card-body"> <span class="timeago small" > Jan 15, 2021 <i class="unloaded">2021-01-15T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bayesian Inference with PyMC3: pt 1 posterior distributions</h3><div class="text-muted small"><p> Jupyter notebook here Introduction Here we use PyMC3 on two Bayesian inference case studies: coin-toss and Insurance Claim occurrence. My last post was an introduction to Baye’s theorem and Ba...</p></div></div></a></div><div class="card"> <a href="/posts/pymc3-predictions/"><div class="card-body"> <span class="timeago small" > Feb 20, 2021 <i class="unloaded">2021-02-20T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bayesian Inference with PyMC3: pt 2 making predictions</h3><div class="text-muted small"><p> Jupyter notebook here In this post I will show how Bayesian inference is applied to train a model and make predictions on out-of-sample test data. For this, we will build two models using a case s...</p></div></div></a></div><div class="card"> <a href="/posts/glm-deep-dive/"><div class="card-body"> <span class="timeago small" > May 17, 2020 <i class="unloaded">2020-05-17T19:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>A deep dive on GLM's in frequency severity models</h3><div class="text-muted small"><p> Jupyter notebook here This notebook is a deep dive into General Linear Models (GLM’s) with a focus on the GLM’s used in insurance risk modeling and pricing (Yan, J. 2010).I have used GLM’s befor...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/finCEN/" class="btn btn-outline-primary" prompt="Older"><p>The finCEN files: Uncovering money laundering patterns in the global banking network</p></a> <a href="/posts/bayesian-inference-pymc3/" class="btn btn-outline-primary" prompt="Newer"><p>Bayesian Inference with PyMC3: pt 1 posterior distributions</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//https-bpostance-github-io.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'Bayesian Inference by hand'; this.page.url = 'https://bpostance.github.io/posts/introduction-to-bayesian-inference/'; this.page.identifier = '/posts/introduction-to-bayesian-inference/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/username">BenPostance</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."> Some rights reserved. </span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/bayes-theory/">bayes theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/data-mining/">data mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial analysis</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://bpostance.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
