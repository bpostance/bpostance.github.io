<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="A guide to clustering large datasets with mixed data-types [updated]" /><meta name="author" content="Ben Postance" /><meta property="og:locale" content="en_US" /><meta name="description" content="Risk, Data Science and Machine Learning" /><meta property="og:description" content="Risk, Data Science and Machine Learning" /><link rel="canonical" href="https://bpostance.github.io/posts/clustering-mixed-data/" /><meta property="og:url" content="https://bpostance.github.io/posts/clustering-mixed-data/" /><meta property="og:site_name" content="Ben Postance" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-03-25T18:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="A guide to clustering large datasets with mixed data-types [updated]" /><meta name="twitter:site" content="@benpostance" /><meta name="twitter:creator" content="@Ben Postance" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Postance"},"dateModified":"2021-04-19T20:54:48+01:00","datePublished":"2021-03-25T18:00:00+00:00","description":"Risk, Data Science and Machine Learning","headline":"A guide to clustering large datasets with mixed data-types [updated]","mainEntityOfPage":{"@type":"WebPage","@id":"https://bpostance.github.io/posts/clustering-mixed-data/"},"url":"https://bpostance.github.io/posts/clustering-mixed-data/"}</script><title>A guide to clustering large datasets with mixed data-types [updated] | Ben Postance</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> // see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> MathJax = { tex: { inlineMath: [ // start/end delimiter pairs for in-line math ['$','$'], ['\\(','\\)'] ], displayMath: [ // start/end delimiter pairs for display math ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-140894462-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-140894462-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="top"></div><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/7165735?s=460&u=7ca34b8152bbff087bfc860cc87a4c35b4f9cb1b&v=4" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Ben Postance</a></div><div class="site-subtitle font-italic">Risk, Data Science and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/bpostance" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/benpostance" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['benpostance','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>A guide to clustering large datasets with mixed data-types [updated]</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>A guide to clustering large datasets with mixed data-types [updated]</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Ben Postance </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Mar 25, 2021, 6:00 PM +0000" prep="on" > Mar 25, 2021 <i class="unloaded">2021-03-25T18:00:00+00:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Apr 19, 2021, 8:54 PM +0100" prefix="Updated " > Apr 19, 2021 <i class="unloaded">2021-04-19T20:54:48+01:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6409 words">35 min</span></div></div><div class="post-content"><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2020-02-20-clustering-fig1.png" alt="drawing" width="800" height="350" /></p><p><strong><a href="https://github.com/bpostance/dsc.learn/blob/main/ML/2.3_Clustering/1.1-Clustering-Mixed-Data.ipynb">Jupyter notebook here</a></strong></p><h1 id="a-guide-to-clustering-large-datasets-with-mixed-data-types">A guide to clustering large datasets with mixed data-types</h1><p><em><strong>Pre-note</strong> If you are an early stage or aspiring data analyst, data scientist, or just love working with numbers clustering is a fantastic topic to start with. In fact, I actively steer early career and junior data scientist toward this topic early on in their training and continued professional development cycle.</em></p><p><em>Learning how to apply and perform accurate clustering analysis takes you though many of the core principles of data analysis, mathematics, machine learning, and computational science. From learning about data types and geometry, confusion matrix, to applying iterative aglorithms and efficient computation on big data. These foundational concepts crop up in most other areas of data science and machine learning. For instance, cluster distance matrices underpin and, mathematically, are near identical to graph data structures used in deep learning graph neural networks at the cutting edge of artificial intelligence research. So if you are just starting then don’t be put of and read on regardless of your level. We all have to start somewhere and this is a very good place!</em></p><h1 id="1-introduction">1. Introduction</h1><p>Cluster analysis is the task of grouping objects within a population in such a way that objects in the same group or cluster are more similar to one another than to those in other clusters. Clustering is a form of unsupervised learning as the number, size and distribution of clusters is unknown a priori. Clustering can be applied to a variety of different problems and domains including: customer segmentation for retail sales and marketing, identifying higher or lower risk groups within <a href="https://www.casact.org/pubs/dpp/dpp08/08dpp170.pdf">insurance portfolios</a>, to finding <a href="https://astronomycommunity.nature.com/users/253561-ingo-waldmann/posts/48323-deep-learning-saturn">storm systems on Jupyter</a>, and even identifying <a href="https://arxiv.org/abs/1404.3097">galaxies far far away</a>.</p><p>Many real-world datasets include combinations of numerical, ordinal (e.g. small, medium, large), and nominal (e.g. France, China, India) data features. However, many popular clustering algorithms and tutorials such as K-means are suitable for numerical data types only. This article is written on the assumption that these methods are familiar - but otherwise Sklearn provides an excellent review of these methods <a href="https://scikit-learn.org/stable/modules/clustering.html#clustering">here</a> for a quick refresher.</p><p>This article seeks to provide a review of methods and a practical application for clustering a dataset with mixed datatypes.</p><h2 id="11-aim">1.1 Aim:</h2><p>To evaluate methods to cluster datasets containing a variety of datatypes.</p><h2 id="12-objectives">1.2 Objectives:</h2><ol><li>To research and review clustering techniques for mixed datatype datasets.<li>To research and review feature encoding and engineering strategies.<li>To apply and review clustering methods on a test dataset.</ol><h1 id="2-case-study-auto-insurance-claims">2. Case Study: auto-insurance claims</h1><p>The California auto-insurance claims <a href="https://www.kaggle.com/xiaomengsun/car-insurance-claim-data">dataset</a> contains 8631 observations with two dependent predictor variables Claim Occured and Claim Amount, and 23 independent predictor variables. The <a href="https://rpubs.com/data_feelings/msda_data621_hw4">data dictionary</a> describe each variable including:</p><ul><li>Bluebook = car re-sale value.<li>MVR_PTS = <a href="https://www.wnins.com/losscontrolbulletins/MVREvaluation.pdf">MotorVehicleRecordPoints (MVR) </a> details an individual’s past driving history indicating violations and accidents over a specified period<li>TIF = Time In Force / customer lifetime<li>YOJ = years in job<li>CLM_FRQ = # of claims in past 5 years<li><p>OLDCLAIM = sum $ of claims in past 5 years</p><ul><li>https://community.alteryx.com/t5/Alteryx-Designer-Discussions/Insurance-Datasets/td-p/440035<li>https://rpubs.com/data_feelings/msda_data621_hw4<li>https://rdrr.io/cran/HDtweedie/man/auto.html<li>https://cran.r-project.org/web/packages/insuranceData/insuranceData.pdf</ul></ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="c1"># load data
</span><span class="n">DATA_PATH</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="nf">getcwd</span><span class="p">(),</span><span class="sh">'</span><span class="s">../_data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">raw</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">DATA_PATH</span><span class="p">,</span><span class="sh">'</span><span class="s">car_insurance_claim.csv</span><span class="sh">'</span><span class="p">),</span><span class="n">low_memory</span><span class="o">=</span><span class="bp">False</span><span class="p">,)</span>

<span class="c1"># convert numerical-object to numericals
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">INCOME</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">HOME_VAL</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">BLUEBOOK</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">OLDCLAIM</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">CLM_AMT</span><span class="sh">'</span><span class="p">,]:</span>
    <span class="n">raw</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">[^.0-9]</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">regex</span><span class="o">=</span><span class="bp">True</span><span class="p">,).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">).</span><span class="nf">fillna</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

<span class="c1"># clean textual classes
</span><span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">raw</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="sh">'</span><span class="s">object</span><span class="sh">'</span><span class="p">).</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">raw</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nb">str</span><span class="p">.</span><span class="nf">upper</span><span class="p">().</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">Z_</span><span class="sh">'</span><span class="p">,</span><span class="sh">''</span><span class="p">,</span><span class="n">regex</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="s">[^A-Z]</span><span class="sh">'</span><span class="p">,</span><span class="sh">''</span><span class="p">,</span><span class="n">regex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
<span class="n">data_types</span> <span class="o">=</span> <span class="p">{</span><span class="n">f</span><span class="p">:</span><span class="n">t</span> <span class="k">for</span> <span class="n">f</span><span class="p">,</span><span class="n">t</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">raw</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span><span class="n">raw</span><span class="p">.</span><span class="n">dtypes</span><span class="p">)}</span>
</pre></table></code></div></div><h1 id="3-method">3. Method</h1><h2 id="31-data-pre-processing">3.1 Data pre-processing</h2><p>Apply processing to correct and handle erroneous values, and rename fields and values to make the data easier to work with. Including:</p><ul><li>remove or fill null values<li>drop irrelevant columns<li>shorten categorical value names</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre><td class="rouge-code"><pre><span class="c1"># copy df
</span><span class="n">df</span> <span class="o">=</span> <span class="n">raw</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="c1"># drop ID and Birth
</span><span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">ID</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">BIRTH</span><span class="sh">'</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># remove all nan values
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">OCCUPATION</span><span class="sh">'</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="sh">'</span><span class="s">OTHER</span><span class="sh">'</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">AGE</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">YOJ</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">CAR_AGE</span><span class="sh">'</span><span class="p">]:</span>
    <span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">fillna</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">mean</span><span class="p">(),</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">if</span> <span class="n">df</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">No NaNs</span><span class="sh">'</span><span class="p">)</span>
    
<span class="n">data_meta</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">nunique</span><span class="p">(),</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">num</span><span class="sh">'</span><span class="p">],</span><span class="n">index</span><span class="o">=</span><span class="bp">None</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="sh">'</span><span class="s">num</span><span class="sh">'</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">()</span>
<span class="n">data_meta</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">num</span><span class="sh">'</span><span class="p">]</span>
<span class="n">data_meta</span><span class="p">[</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">numerical</span><span class="sh">'</span>

<span class="c1"># exclude known numericals
</span><span class="n">data_meta</span><span class="p">.</span><span class="n">loc</span><span class="p">[(</span><span class="n">data_meta</span><span class="p">[</span><span class="sh">'</span><span class="s">num</span><span class="sh">'</span><span class="p">]</span><span class="o">&lt;=</span><span class="mi">15</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="o">~</span><span class="n">data_meta</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">].</span><span class="nf">isin</span><span class="p">([</span><span class="sh">'</span><span class="s">MVR_PTS</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">CLM_FREQ</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">CLAIM_FLAG</span><span class="sh">'</span><span class="p">])),</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical</span><span class="sh">'</span>
<span class="n">data_meta</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data_meta</span><span class="p">[</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">].</span><span class="nf">isin</span><span class="p">([</span><span class="sh">'</span><span class="s">CLM_FREQ</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">CLAIM_FLAG</span><span class="sh">'</span><span class="p">]),</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span><span class="sh">'</span><span class="s">claim</span><span class="sh">'</span>

<span class="n">categorical_features</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">data_meta</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data_meta</span><span class="p">[</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="sh">'</span><span class="s">categorical</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">])</span>
<span class="n">numerical_features</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">data_meta</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data_meta</span><span class="p">[</span><span class="sh">'</span><span class="s">type</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="sh">'</span><span class="s">numerical</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">name</span><span class="sh">'</span><span class="p">])</span>

<span class="c1"># shorten names
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">URBANICITY</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">URBANICITY</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span><span class="sh">'</span><span class="s">HIGHLYURBANURBAN</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">URBAN</span><span class="sh">'</span><span class="p">,</span>
                                           <span class="sh">'</span><span class="s">HIGHLYRURALRURAL</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">RURAL</span><span class="sh">'</span><span class="p">})</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">EDUCATION</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">EDUCATION</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span><span class="sh">'</span><span class="s">HIGHSCHOOL</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">HSCL</span><span class="sh">'</span><span class="p">,</span>
                                         <span class="sh">'</span><span class="s">BACHELORS</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">BSC</span><span class="sh">'</span><span class="p">,</span>
                                         <span class="sh">'</span><span class="s">MASTERS</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">MSC</span><span class="sh">'</span><span class="p">,</span>
                                         <span class="sh">'</span><span class="s">PHD</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">PHD</span><span class="sh">'</span><span class="p">})</span>
<span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">CAR_TYPE</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">CAR_TYPE</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span><span class="sh">'</span><span class="s">MINIVAN</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">MVAN</span><span class="sh">'</span><span class="p">,</span> 
                                       <span class="sh">'</span><span class="s">VAN</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">VAN</span><span class="sh">'</span><span class="p">,</span> 
                                       <span class="sh">'</span><span class="s">SUV</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">SUV</span><span class="sh">'</span><span class="p">,</span>
                                       <span class="sh">'</span><span class="s">SPORTSCAR</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">SPRT</span><span class="sh">'</span><span class="p">,</span>
                                       <span class="sh">'</span><span class="s">PANELTRUCK</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">PTRK</span><span class="sh">'</span><span class="p">,</span> 
                                       <span class="sh">'</span><span class="s">PICKUP</span><span class="sh">'</span><span class="p">:</span><span class="sh">'</span><span class="s">PKUP</span><span class="sh">'</span><span class="p">})</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>No NaNs
</pre></table></code></div></div><h2 id="32-exploratory-data-analysis">3.2 Exploratory Data Analysis</h2><p><strong><em>Categorical feature histograms</em></strong></p><p>Shown below are the histogram of each categorical feature. This illustrates both the number and frequency of each category in the dataset.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_10_0.png" alt="png" /></p><p><strong><em>How are claims distributed amongst the categorical features?</em></strong></p><p>As above, the bar plots again illustrate each categorical feature and value, but now also show how the proportion of claims is distributed to each categorical value. For example, Commericial CAR_USE has a relatively higher proportion of claims than Private car use.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_12_0.png" alt="png" /></p><h2 id="33-clustering">3.3 Clustering</h2><p>Recall that each clustering algorithm is an attempt to create natural groupings of the data. At a high-level, clustering algorithms acheive this using a measure of similarity or distance between each pair of data points, between groups and partitions of points, or between points and groups to a representative central point (i.e. centroid). So while the actual algorithm impementations to achive this vary, in essence they are based on this simple principle of distance.This is illustrated quite nicely in illustration below that shows a data set with 3 clusters, and iterative cluster partitioning a-f by updating the centroid points (Chen 2018). Here the clusters are formed by measuring the distance between each data point (solid fill) and a representative centoid point (hollow fill).</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://www.researchgate.net/profile/Yu-Zhong-Chen/publication/324073652/figure/fig2/AS:611048927277056@1522696825062/A-schematic-illustration-of-the-K-means-algorithm-for-two-dimensional-data-clustering.png" width="450" height="450" /></p><p>Because clustering algorithms utilise this concept of distance both it is crucial to consider both:</p><ul><li>Distance Measures. The distance or “similarity” measure used.<li>Feature Engineering. The nature of our data and the way the data is presented the clustering algorithm.</ul><h3 id="331-distance-measures">3.3.1 Distance Measures</h3><p>Below are some of the common distance measures used for clustering. Computational efficiency is important here as each data feature introduces an additional dimension.For clustering, by definition, we often have multiple features to make sense of and therefore the efficiency of the calculation in high dimensional space is crucial.</p><p><strong><em><a href="https://en.wikipedia.org/wiki/Euclidean_distance#Higher_dimensions">Euclidean distance</a></em></strong> is the absolute numerical difference of their location in Euclidean space. Distances can be 0 or take on any positive real number. It is given by the root sum-of-squares of differences between each pair (p,q) of points. And we can see that for high dimensions we simply add the distance.</p>\[d_n(p,q) = \sqrt{(p_1-q_1)^2+(p_2-q_2)^2...+(p_n-q_n)^2}\]<p><strong><em><a href="https://en.wikipedia.org/wiki/Taxicab_geometry">Manhattan distance</a></em></strong> is again the sum of the absolute numerical difference between two points in space, but using cartesian coordinates. Whilst Euclidean distance is the straight-line “as the crow flies” with Pythagoras theorem, Manhattan takes distance as the sum of the line vectors (p,q).</p>\[d_n(p,q) = \sum_{i=1}^{n} |{(p_n-q_n)}|\]<p>This image illustrates examples for: a) Euclidean space distance, b) Manhattan distance in cartesian coordinate space, and c) both with the green line showing a Euclidean path, while the blue, red, and yellow lines take a cartesian path with Manhattan distance. This illustrates how clustering results may be influenced by distance measures applied and depending on whether the data features are real and numeric or discrete ordinal and categorical values. In addition, perhaps this also illustrates to you how and why geometry and distance are important in other domains such as shortest path problems. <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering/spaces.png" width="550" height="250" /></p><p>There are many distance metrics (e.g. see <a href="http://halweb.uc3m.es/esp/Personal/personas/jmmarin/esp/MetQ/Talk6.pdf">these slides</a>). Minkowski distance for example, is a generalization of both the Euclidean distance and the Manhattan distance. Scipy has a convenient <a href="https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.spatial.distance.pdist.html">pair distance</a> <code class="language-plaintext highlighter-rouge">pdist()</code> function that applies many of the most common measures.</p><blockquote><p><strong>Example</strong>:<br /> <code class="language-plaintext highlighter-rouge"> from scipy.spatial.distance import pdist pdist(df,metric='minkowski') </code></p></blockquote><p>There are also hybrid distance measures. In our case study, and topic of this article, the data contains a mixture of features with different data types and this requires such a measure.</p><p><strong><em><a href="https://www.jstor.org/stable/2528823?seq">Gower (1971) distance</a></em></strong> is a hybrid measure that handles both continuous and categorical data.</p><ul><li>If the data feature are continuous or ordinal, the Manhattan or a ranked ordinal Manhattan is applied respectively.<li>If the data feature are categorical, then a <a href="https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient#Formula">DICE</a> coefficient is applied. DICE is explained <a href="https://stats.stackexchange.com/a/55802/100439">here</a>. However, If you are familiar with Jaccard coefficient and or binary classification (e.g. True Positives TP and False Positives FP etc) and confusion matrices then DICE is going to be familiar as</ul>\[DICE = \frac{2|X \cap Y|}{|X|+|Y|} = \frac{2TP}{2TP+FP+FN}\]<p>The Gower distance of a pair of points $G(p,q)$ then is:</p>\[G_n(p,q) = \frac{\sum_{i=1}^{n}W_{pqk}S_{pqk}}{\sum_{i=1}^{n}W_{pqk}}\]<p>where $S_{pqk}$ is either the Manhattan or DICE value for feature $k$, and $W_{pqk}$ is either 1 or 0 if $k$ feature is valid. Its the sum of feature scores divided by the sum of feature weights.</p><h3 id="332-feature-engineering">3.3.2 Feature Engineering</h3><p>With this improved understanding of the clustering distance measures it is now clear that the scale of our data features is equally important. For example, imagine we were clustering cars by their performance and weight characteristics.</p><ul><li>Car A does 0-60 mph in 3.5 seconds and has mass 3000 KG<li>Car B does 0-60 mph in 5.5 seconds and has mass 5000 KG</ul><p>The feature distances between Car A and B are 2.0 seconds and 2000 KG. Thus our 2000 unit distance for mass is orders of magnitude higher than 2.0 seconds for 0-60 mph. Clustering data in this form would yield results bias toward high range features (see more examples in these StackOverflow answers <a href="https://datascience.stackexchange.com/questions/22795/do-clustering-algorithms-need-feature-scaling-in-the-pre-processing-stage/22915">1</a>,<a href="https://stats.stackexchange.com/a/7182/100439">2</a>,<a href="https://stats.stackexchange.com/questions/385775/normalizing-vs-scaling-before-pca">3</a>). When using any algorithm that computes distance or assumes normally distributed data, scale your features <a href="https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e">4</a>.</p><p>For datasets with mixed data types consider you have scaled all features to between 0-1. This will ensure distance measures are applied uniformly to each feature. The numerical features will have distances with min-max 0-1 and real numbers between e.g. 0.1,0.2,0.5,…0.99. Whereas the distances for categorical features be values of either 0 or 1. As for mass KG in the car example above, this could still lead to a bias in the formation of clusters toward categorical feature groups as their distances are always either the min-max value of 0 or 1.</p><p>Selecting the appropriate transformations and scaling to apply is part science and part art. There are often several strategies that may suit and must be applied and evaluated in relation to the context of challenge at hand, the data and its domain. Crucially, whatever strategy is adopted, <strong>all features</strong> in the final dataset that is used for clustering data must be on the same scale for each feature to be treated equally by the distance metrics.</p><p><strong><em>Different data types and how to handle them</em></strong></p><p>Here are two excellent articles and figures that delve deeper on (left) <a href="https://towardsdatascience.com/data-types-in-statistics-347e152e8bee">data-types</a> and (right) <a href="https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02">encoding techniques</a>.</p><p align="middle"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://miro.medium.com/max/700/1*dvvxoZTdewLFs3RyZTJreA.png" width="30%" /> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="https://innovation.alteryx.com/content/images/2019/08/categorical-encoding-01-01.png" width="30%" /></p><p>In summary, these are:</p><p><strong><em>Numerical features:</em></strong> continuous and interval features such as mass, age, speed, luminosity, friction.</p><ul><li>Use ratios and percentages.<li>normalisation (i.e. normalise values to be on scale of 0-1)<li>standardisation (i.e. how many standard deviations the value is from the sample mean)<li>transformation (i.e. log transformation).</ul><p><strong><em>Nominal Categorical features:</em></strong> Unordered nominal or binary symmetric values where outcomes are of equal importance (e.g. Male or Female).</p><ul><li>One-hot and dummy encoding (i.e. create binary indicator of each category).<li>If handling a feature with high cardinality &gt;15, try to reduce dimensionality by feature engineering or apply binary or hash encoding.</ul><p><strong><em>Ordinal Categorical features:</em></strong> Ordered ordinal or binary asymmetric values, where outcomes are not of equal importance.</p><blockquote><p><strong>Example</strong>: consider the ordinal categorical feature for olympic medals encoded as Bronze=1, Silver=2, Gold=3. Here bronze (= 1 * 1), silver (= 2 * bronze), gold (= 3 * bronze). The difference between Gold-Bronze (= 3-1 = 2) is greater than between Gold and Silver (= 3-2 = 1) worse. Clearly this is ordinal as each category has a rank and relative scale compared to each other category.</p><ul><li>Label encoding with 0-1 normalisation if values are of equal-importance and on an increasing scale.<li>Rank values with 0-1 normalisation, again if values are on equal-importance increasing scale.<li>If there is a binary target variable in the dataset (e.g. event occurrence, medical diagnosis, iris type), one can also assign frequencies, odd ratios, or weights-of-evidence to each ordinal class.</ul></blockquote><p>By far ordinal data is the most challenging to handle. There are many arguments between mathematical purists, statisticians and other data practitioners on wether to treat ordinal data as qualitatively or quantitatively (<a href="https://creativemaths.net/blog/ordinal/">see here</a>).</p><blockquote><p><strong>NOTE</strong>: Distance measures and encoding is perhaps THE hardest part to clustering data. In this authors opinion, especially for categorical and ordinal features, data should be treated with caution and practitioners should trial and evaluate different feature engineering and encoding strategies based on their understanding of the data and its domain. My other recommendation is to trial and test different strategies first with smaller datasets and those where you have a reasonable understanding of what the clusters SHOULD be (Iris, Titanic). Valid your approach on these data, and then apply to you own data and build up the complexity from there deciding where to put more focus on tuning the encoding process. Here are some more useful links: <br /></p><ul><li><a href="https://www.quora.com/What-are-binary-symmetric-and-asymmetric-attributes">Binary symmetric and assymetric variables</a><li><a href="https://paginas.fe.up.pt/~ec/files_0506/slides/05_Clustering.pdf">datatype conversions in clustering</a><li><a href="https://towardsdatascience.com/normalization-vs-standardization-quantitative-analysis-a91e8a79cebf">Normalization vs Standardization — Quantitative analysis</a><li><a href="https://stats.stackexchange.com/a/10291/100439">Normalization vs Standardization</a></ul></blockquote><h3 id="333-clustering-methods">3.3.3 Clustering Methods</h3><p>There are also considerations for the clustering algorithm that is applied. These will be introduced and discussed on application to the case study dataset in the results section.</p><h1 id="4-results">4. Results</h1><h2 id="41-feature-engineering">4.1 Feature Engineering</h2><p><strong><em>Scaling numerical features</em></strong></p><p>Below, both Standard and MinMax scaling is applied to show how the data is transformed. The MinMax scaled data is used going forward.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span><span class="p">,</span><span class="n">RobustScaler</span><span class="p">,</span><span class="n">StandardScaler</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">df</span><span class="p">[</span><span class="n">numerical_features</span><span class="p">][:</span><span class="mi">2</span><span class="p">]</span>
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>MVR_PTS<th>YOJ<th>TIF<th>CAR_AGE<th>AGE<th>TRAVTIME<th>CLM_AMT<th>BLUEBOOK<th>OLDCLAIM<th>HOME_VAL<th>INCOME<tbody><tr><th>0<td>3<td>11.0<td>11<td>18.0<td>60.0<td>14<td>0.0<td>14230.0<td>4461.0<td>0.0<td>67349.0<tr><th>1<td>0<td>11.0<td>1<td>1.0<td>43.0<td>22<td>0.0<td>14940.0<td>0.0<td>257252.0<td>91449.0</table></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="c1"># Standard Scaled "mean normalisation"
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_features</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Means:</span><span class="se">\t</span><span class="sh">"</span><span class="p">,[</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">x</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">scaler</span><span class="p">.</span><span class="n">mean_</span><span class="p">])</span>
<span class="n">numerical_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_features</span><span class="p">])</span>
<span class="n">numerical_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">numerical_data</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">numerical_features</span><span class="p">)</span>
<span class="n">numerical_data</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Feature Means:	 ['1.7', '1e+01', '5.3', '8.3', '4.5e+01', '3.3e+01', '1.5e+03', '1.6e+04', '4e+03', '1.5e+05', '5.8e+04']
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>MVR_PTS<th>YOJ<th>TIF<th>CAR_AGE<th>AGE<th>TRAVTIME<th>CLM_AMT<th>BLUEBOOK<th>OLDCLAIM<th>HOME_VAL<th>INCOME<tbody><tr><th>0<td>0.597453<td>0.131552<td>1.379567<td>1.753103<td>1.762458<td>-1.223551<td>-0.319843<td>-0.169656<td>0.048899<td>-1.118502<td>0.190439<tr><th>1<td>-0.792137<td>0.131552<td>-1.053171<td>-1.318759<td>-0.213574<td>-0.719421<td>-0.319843<td>-0.085417<td>-0.461938<td>0.853671<td>0.690195</table></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># MinMax scaled
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_features</span><span class="p">])</span>
<span class="n">numerical_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">numerical_features</span><span class="p">])</span>
<span class="n">numerical_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">numerical_data</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">numerical_features</span><span class="p">)</span>
<span class="n">numerical_data</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>MVR_PTS<th>YOJ<th>TIF<th>CAR_AGE<th>AGE<th>TRAVTIME<th>CLM_AMT<th>BLUEBOOK<th>OLDCLAIM<th>HOME_VAL<th>INCOME<tbody><tr><th>0<td>0.230769<td>0.478261<td>0.416667<td>0.677419<td>0.676923<td>0.065693<td>0.0<td>0.186547<td>0.078212<td>0.000000<td>0.183497<tr><th>1<td>0.000000<td>0.478261<td>0.000000<td>0.129032<td>0.415385<td>0.124088<td>0.0<td>0.196952<td>0.000000<td>0.290588<td>0.249159</table></div><p><strong><em>Sacling categorical features: ordinal &amp; nominal</em></strong></p><p>Examining our EDA above plots in section 3.2, we may want to combine some classes where there are low target frequencies and or high cardinality. These are:</p><ul><li>KIDSDRIV: collapse &gt;= 2 to single category<li>HOMEKIDS: collapse &gt;= 4 to single category</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1"># you can use map
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">KIDSDRIV</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">KIDSDRIV</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>
<span class="c1"># or pd.cut()
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">HOMEKIDS</span><span class="sh">'</span><span class="p">]</span><span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">HOMEKIDS</span><span class="sh">'</span><span class="p">],</span>
                       <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span>
                       <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],</span>
                       <span class="n">include_lowest</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                       <span class="n">right</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</pre></table></code></div></div><p>Remember, nominal features are categoricals with values that have no order. Here, we could argue that EDUCATION and OCCUPATION are either nominal (i.e. unordered) or ordinal (i.e. ordered). This could be quite a debate….but there is a good argument to treat EDUCATION as ordinal and OCCUPATION as a categorical nominal. KIDSDRIV and HOMEKIDS can also be considered ordered and increasing in scale, however the adjustments we just made to the upper values are not symtetric in scale. This could be OK though given that there is very little data for the upper categories.</p><p>Let’s keep them as they are here. I will also apply MinMax scaling with range 0-1.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="c1"># create some lists
</span><span class="n">ordinal_features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">EDUCATION</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">KIDSDRIV</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">HOMEKIDS</span><span class="sh">'</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Ordinals:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">ordinal_features</span><span class="p">)</span>

<span class="c1"># apply order to education
</span><span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">EDUCATION</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">'</span><span class="s">EDUCATION</span><span class="sh">'</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span><span class="sh">'</span><span class="s">HSCL</span><span class="sh">'</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">BSC</span><span class="sh">'</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="sh">'</span><span class="s">MSC</span><span class="sh">'</span><span class="p">:</span><span class="mi">2</span><span class="p">,</span><span class="sh">'</span><span class="s">PHD</span><span class="sh">'</span><span class="p">:</span><span class="mi">3</span><span class="p">})</span>
<span class="n">ordinal_data</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">ordinal_features</span><span class="p">]</span>

<span class="c1"># MinMax scaled
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">scaler</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">ordinal_features</span><span class="p">])</span>
<span class="n">ordinal_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">ordinal_features</span><span class="p">])</span>
<span class="n">ordinal_data</span><span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">ordinal_data</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">index</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">ordinal_features</span><span class="p">)</span>
<span class="n">ordinal_data</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Ordinals:
 ['EDUCATION', 'KIDSDRIV', 'HOMEKIDS']
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>EDUCATION<th>KIDSDRIV<th>HOMEKIDS<tbody><tr><th>0<td>1.0<td>0.0<td>0.0<tr><th>1<td>0.0<td>0.0<td>0.0</table></div><p>For nominals that are binary features you can leave them as a single series indicator (e.g. Red Car Yes (1) or No (0)) or split them to two mutually exclusive one-hot-encoded series (e.g RedCarYes and RedCarNo). For some methods such as XGBoost the latter approach is proffered.</p><p>Below I have written a for loop that keeps a single series for binary features and creates n-dimensional features for nominal features with n+2 values.</p><p>No need to scale here as all the values are either 0 or 1.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="c1"># For the nominals one-hot-encoding is applied
</span><span class="n">nominal_features</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">categorical_features</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ordinal_features</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Nominals:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span><span class="n">nominal_features</span><span class="p">)</span>

<span class="n">nominal_data</span> <span class="o">=</span> <span class="nf">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">x</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="n">nominal_features</span><span class="p">].</span><span class="nf">nunique</span><span class="p">().</span><span class="nf">iteritems</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">nominal_data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nf">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span><span class="n">drop_first</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">x</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">nominal_data</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nf">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="n">i</span><span class="p">]],</span><span class="n">drop_first</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
        
<span class="n">nominal_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span><span class="n">nominal_data</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Nominals:
 ['CAR_USE', 'REVOKED', 'RED_CAR', 'GENDER', 'MSTATUS', 'URBANICITY', 'PARENT1', 'CAR_TYPE', 'OCCUPATION']
</pre></table></code></div></div><p><strong><em>Final Datasets used for clustering</em></strong></p><p>I will create two copies of the data:</p><ol><li>using the above OHE transformations and feature scaling (0,1).<li>applying the above transformations but without feature scaling. The reason for this one is because some distance and clustering packages handle the preparation for you - but its good to know how to do it yourself.</ol><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># transformed and scaled dataset
</span><span class="n">Xy_scaled</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">numerical_data</span><span class="p">,</span><span class="n">nominal_data</span><span class="p">,</span><span class="n">ordinal_data</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Data min:max </span><span class="si">{</span><span class="n">Xy_scaled</span><span class="p">.</span><span class="nf">min</span><span class="p">().</span><span class="nf">min</span><span class="p">(),</span><span class="n">Xy_scaled</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">max</span><span class="p">()</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># original data
</span><span class="n">Xy_original</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">numerical_features</span><span class="o">+</span><span class="n">nominal_features</span><span class="o">+</span><span class="n">ordinal_features</span><span class="p">].</span><span class="nf">copy</span><span class="p">()</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Data min:max (0.0, 1.0)
</pre></table></code></div></div><h2 id="42-clustering-methods-for-mixed-datatypes">4.2 Clustering methods for mixed datatypes</h2><p>In this section several clustering algorithms and approaches are applied. However, first, some generic cluster evaluation techniques are introduced.</p><h3 id="evaluation-techniques">Evaluation techniques:</h3><p><strong><em>Elbow plots, Silhouette Scores, and Silhouette Samples</em></strong></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>
<span class="kn">from</span> <span class="n">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_samples</span><span class="p">,</span> <span class="n">silhouette_score</span>
</pre></table></code></div></div><p>To illustrate the evaluation techniques we will generate some simple clusters using sklearn <code class="language-plaintext highlighter-rouge">make_blobs</code>.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">Xy_</span><span class="p">,</span><span class="n">clusters_</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">df_</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">Xy_</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">])</span>
<span class="n">df_</span><span class="p">[</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">clusters_</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">X</span><span class="sh">'</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="sh">'</span><span class="s">Y</span><span class="sh">'</span><span class="p">,</span><span class="n">hue</span><span class="o">=</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">df_</span><span class="p">);</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_32_0.png" alt="png" /></p><p>To help evaluate and identify the optimal number of clusters $k$ in a single plot, one can apply the following methods:</p><ul><li><p>The clustering metric Elbow technique. We can directly use the clustering measure to evaluate the cluster results. For k-means, clusters are created by minimising Intertia (see 4.2.1 below). The so called <a href="https://medium.com/@jyotiyadav99111/selecting-optimal-number-of-clusters-in-kmeans-algorithm-silhouette-score-c0d9ebb11308">“elbow technique”</a> plots intertia and the optimal number of clusters $k$ is selected as the point where intertia is inflected. The left plot below indicates that intertia stops changing substantially aroun $k=5$, the true number of clusters.</p><li><p>The <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score">silhouette score</a> is a measure of cluster seperation and similarity. The score combines the the average intra-cluster difference and the nearest-cluster distance of each sample. With 1 being best “all cluster samples are similar” and -1 being worst “clusters are dissimila and overlapping”. The sklearn <code class="language-plaintext highlighter-rouge">silhouette_score()</code> function can in-fact calculate any pairwise distance metric from sklearn or <code class="language-plaintext highlighter-rouge">scipy.spatial.distance</code>. For KMeans, Euclidean distance is the default score but we can also use Correlation, Manhattan, etc metrics. As for the Elbow approach, $k$ is selected that optimises the selected metric.</p></ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="n">results</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
<span class="n">k_cand</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span> <span class="c1">#
</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_cand</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">Xy_</span><span class="p">)</span>
    <span class="n">score0</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span>
    <span class="n">score1</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">Xy_</span><span class="p">,</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">Euclidean</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">score2</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">Xy_</span><span class="p">,</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">correlation</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span><span class="n">kmeans</span><span class="p">,</span><span class="sh">'</span><span class="s">s0</span><span class="sh">'</span><span class="p">:</span><span class="n">score0</span><span class="p">,</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">:</span><span class="n">score1</span><span class="p">,</span><span class="sh">'</span><span class="s">s2</span><span class="sh">'</span><span class="p">:</span><span class="n">score2</span><span class="p">}</span>

<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s0</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Inertia</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Euclidean</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s2</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Correlation</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">k_cand</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_34_0.png" alt="png" /></p><p>Selecting the optimal value for $k$ can also be aided by closer inspecting the samples and clusters themselves. Sklearn has a nice tutorial <a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py">here</a>. These include:</p><ul><li><p><a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_samples.html#sklearn.metrics.silhouette_samples">Silhouette Samples</a> show the per data point silhouette score wihtin a cluster (left). The coloured bars are the data points within eac cluster, where width is the data point silhouette score. The vertical dashed line is the average score per data point across all clusters. The cluster bard group height indicates the size of the cluster. Clusters and samples with “noisy” silhouette plots in terms of unequal widths and heights indicates poor clustering.</p><li><p>Cluster centroids (right). Accompanying the silhouette sample plots, the data clusters and centroids are shown explicitly.</p></ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre><td class="rouge-code"><pre><span class="n">k_cand</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span>

<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">k_cand</span><span class="p">),</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>

<span class="k">for</span> <span class="n">e</span><span class="p">,</span><span class="n">k</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">k_cand</span><span class="p">):</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">Xy_</span><span class="p">)</span>
    
    <span class="n">cdict</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">cm</span><span class="p">.</span><span class="nc">Set1</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">)}</span>
    
    <span class="c1">## silhouette samples
</span>    <span class="n">silhouette_vals</span> <span class="o">=</span> <span class="nf">silhouette_samples</span><span class="p">(</span><span class="n">Xy_</span><span class="p">,</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">)</span>
    <span class="n">y_lower</span> <span class="o">=</span> <span class="mi">0</span> 
    <span class="n">y_upper</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">cluster</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">)):</span>
        <span class="n">cluster_silhouette_vals</span> <span class="o">=</span> <span class="n">silhouette_vals</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="o">==</span><span class="n">cluster</span><span class="p">]</span>
        <span class="n">cluster_silhouette_vals</span><span class="p">.</span><span class="nf">sort</span><span class="p">()</span>
        <span class="n">y_upper</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">cluster_silhouette_vals</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">barh</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">y_lower</span><span class="p">,</span><span class="n">y_upper</span><span class="p">),</span>
                      <span class="n">cluster_silhouette_vals</span><span class="p">,</span>
                      <span class="n">height</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                      <span class="n">color</span><span class="o">=</span><span class="n">cdict</span><span class="p">[</span><span class="n">cluster</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.03</span><span class="p">,(</span><span class="n">y_lower</span><span class="o">+</span><span class="n">y_upper</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span><span class="nf">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        
        <span class="n">y_lower</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">cluster_silhouette_vals</span><span class="p">)</span> <span class="c1"># add for next iteration 
</span>        <span class="n">avg_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">silhouette_vals</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">axvline</span><span class="p">(</span><span class="n">avg_score</span><span class="p">,</span><span class="n">linestyle</span> <span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_yticks</span><span class="p">([])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette coefficient values</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Cluster labels</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Silhouette plot for the various clusters</span><span class="sh">'</span><span class="p">)</span>
        
    <span class="c1">## plot data and cluster centroids
</span>    <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">Xy_</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span>
    <span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">):</span> <span class="c1"># plot data by cluster
</span>        <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">results</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">cluster</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                         <span class="n">y</span><span class="o">=</span><span class="n">results</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">cluster</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                         <span class="n">color</span><span class="o">=</span><span class="n">cdict</span><span class="p">[</span><span class="n">cluster</span><span class="p">],</span>
                         <span class="n">label</span><span class="o">=</span><span class="n">cluster</span><span class="p">)</span>
    <span class="c1"># plot centroids
</span>    <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
                     <span class="n">y</span><span class="o">=</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
                     <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">180</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">1</span><span class="p">].</span><span class="nf">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">e</span><span class="p">,</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">KMeans</span><span class="se">\n</span><span class="s">$k$ = </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_36_0.png" alt="png" /></p><p>For the scatter plots. If you have multiple features in your clustering data, or otherwise do not have the distance matrix, then first you must apply a decomposition technique such as PCA, MCA, or t-SNE to obtain a 2 or 3 dimensional vectors for plotting.</p><blockquote><p><strong><em>NOTE</em></strong> use t-SNE with caution as this alters the scale and magnitude of the feature spaces and some methods, such as plotting centroids, will not work as shown below. See <a href="https://stats.stackexchange.com/questions/271705/what-is-the-good-use-for-t-sne-apart-from-data-visualization">here</a> and <a href="https://stats.stackexchange.com/questions/263539/clustering-on-the-output-of-t-sne">here</a> for explanations and guidance using t-SNE.</p></blockquote><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="n">a</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">4000</span><span class="p">,</span><span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">tsne_model</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">tsne_model</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">tsne</span><span class="p">)</span>
<span class="n">tsne</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span>

<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">):</span> <span class="c1"># plot data by cluster
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">tsne</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">tsne</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">cluster</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">tsne</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">tsne</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">cluster</span><span class="p">)[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">color</span><span class="o">=</span><span class="n">cdict</span><span class="p">[</span><span class="n">cluster</span><span class="p">],</span>
                <span class="n">label</span><span class="o">=</span><span class="n">cluster</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
            <span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
            <span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">180</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">centroids</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre>[t-SNE] Computing 91 nearest neighbors...
[t-SNE] Indexed 4000 samples in 0.005s...
[t-SNE] Computed neighbors for 4000 samples in 0.132s...
[t-SNE] Computed conditional probabilities for sample 1000 / 4000
[t-SNE] Computed conditional probabilities for sample 2000 / 4000
[t-SNE] Computed conditional probabilities for sample 3000 / 4000
[t-SNE] Computed conditional probabilities for sample 4000 / 4000
[t-SNE] Mean sigma: 0.504213
[t-SNE] KL divergence after 250 iterations with early exaggeration: 68.919662
[t-SNE] KL divergence after 500 iterations: 1.553155
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_38_1.png" alt="png" /></p><h3 id="421-k-means">4.2.1 K-Means</h3><p>The KMeans algorithm attempts to cluster data by creating groups that minimise the within-cluster sum-of-squared differences, aka inertia. Inertia measures the similarity of each pair of points as the Euclidean distance between them, smaller distances == higher similarity. The KMeans algorithm is computed in 3 steps:</p><ol><li>$k_n$ initial random centroids are chosen.<li>data points are assigned to their nearest $k_i$ centroid.<li>new centroids are created as the mean location of $k_i$ and its assigned points. Step 2 and 3 are repeated on the new cetroids until there is little to no change between iterations.</ol><p>KMeans works well on large datasets with numerical features, uniform cluster size, and where clusters are convex and isotropic “blobs”. For a full description of KMeans and its limitations see <a href="https://scikit-learn.org/stable/modules/clustering.html#k-means">sklearn KMeans documentation</a>.</p><p>For high-dimensional data (e.g. lots of features) and with mixed data-types, there is a high likelihood that the Intertia and Euclidean distances become insignificant due to the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions">curse of high dimensionality</a>. Simply put, in high-dimensional space and with independantly distributed features (i.e. no correlation) it is typical that the pairwise distances between any and all points becomes very small. This could well be a problem for our data set with categorical features.</p><p>Applying KMeans to our scaled dataset shows there may be a high number of clusters. This may reflect some actual clusters within the data. But it may also be an artefact of having mixed numerical and categorical features and limitations of KMeans given high dimensionality of the data.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="n">results</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
<span class="n">k_cand</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">k_cand</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">50</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_cand</span><span class="p">:</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="nc">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">fit</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">)</span>
    <span class="n">score0</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">inertia_</span>
    <span class="n">score1</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">,</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">Euclidean</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">score2</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">,</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">correlation</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span><span class="n">kmeans</span><span class="p">,</span><span class="sh">'</span><span class="s">s0</span><span class="sh">'</span><span class="p">:</span><span class="n">score0</span><span class="p">,</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">:</span><span class="n">score1</span><span class="p">,</span><span class="sh">'</span><span class="s">s2</span><span class="sh">'</span><span class="p">:</span><span class="n">score2</span><span class="p">}</span>

<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s0</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Inertia</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Euclidean</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s2</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Correlation</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span> <span class="ow">in</span> <span class="n">axs</span><span class="p">:</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">k_cand</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_40_0.png" alt="png" /></p><p>As $k$ increases inertia and silhouette scoring is shown to improve. There are simply too many $k$ to make silhouette sampling and centroid plotting viable, however we can plot this using t-SNE to visually inspect what is happening. Albeit crudely. There seems to be a degree of good seperation in the t-SNE plot but also with some larger blobs of mixed colours.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">tsne_model</span> <span class="o">=</span> <span class="nc">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">tsne_model</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">)</span>
<span class="n">tsne</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">tsne</span><span class="p">)</span>
<span class="n">tsne</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">400</span><span class="p">][</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">].</span><span class="n">labels_</span>

<span class="k">for</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">):</span> <span class="c1"># plot data by cluster
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">tsne</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">tsne</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">cluster</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">tsne</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">tsne</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span><span class="o">==</span><span class="n">cluster</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
               <span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre>[t-SNE] Computing 91 nearest neighbors...
[t-SNE] Indexed 10302 samples in 0.001s...
[t-SNE] Computed neighbors for 10302 samples in 2.468s...
[t-SNE] Computed conditional probabilities for sample 1000 / 10302
[t-SNE] Computed conditional probabilities for sample 2000 / 10302
[t-SNE] Computed conditional probabilities for sample 3000 / 10302
[t-SNE] Computed conditional probabilities for sample 4000 / 10302
[t-SNE] Computed conditional probabilities for sample 5000 / 10302
[t-SNE] Computed conditional probabilities for sample 6000 / 10302
[t-SNE] Computed conditional probabilities for sample 7000 / 10302
[t-SNE] Computed conditional probabilities for sample 8000 / 10302
[t-SNE] Computed conditional probabilities for sample 9000 / 10302
[t-SNE] Computed conditional probabilities for sample 10000 / 10302
[t-SNE] Computed conditional probabilities for sample 10302 / 10302
[t-SNE] Mean sigma: 0.428354
[t-SNE] KL divergence after 250 iterations with early exaggeration: 65.685791
[t-SNE] KL divergence after 500 iterations: 0.915920
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_42_1.png" alt="png" /></p><p>Herein lies another problem of unsupervised learning - what are the correct clusters? There is no definitive answer in this case. You will have to apply a blend of domain knowledge and expertise with the clustering results and interpretation to derive the most meaningfull insights from the clusters. Here you can utilise other methods, diagnostic statistics, and visualisations, to interpret and assess your clusters. These might include:</p><ul><li>association tests: correlation, cramer’s V, chi-square tests<li>box plotting and distribution tests between clusters<li>classification: build decision tree’s with $k$ as the target and explain clusters using feature importance and shap values</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span><span class="n">sharex</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">Xy_original</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">50</span><span class="p">][</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">].</span><span class="n">labels_</span>

<span class="n">Xy_original</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">sort_index</span><span class="p">().</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Xy_original</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">column</span><span class="o">=</span><span class="sh">'</span><span class="s">AGE</span><span class="sh">'</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Xy_original</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">column</span><span class="o">=</span><span class="sh">'</span><span class="s">INCOME</span><span class="sh">'</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">Xy_original</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">).</span><span class="nf">agg</span><span class="p">([</span><span class="sh">"</span><span class="s">mean</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">median</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">var</span><span class="sh">"</span><span class="p">])[:</span><span class="mi">2</span><span class="p">]</span>
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } .dataframe thead tr:last-of-type th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr><th><th colspan="3" halign="left">MVR_PTS<th colspan="3" halign="left">YOJ<th colspan="3" halign="left">TIF<th>CAR_AGE<th>...<th>INCOME<th colspan="3" halign="left">EDUCATION<th colspan="3" halign="left">KIDSDRIV<th colspan="3" halign="left">HOMEKIDS<tr><th><th>mean<th>median<th>var<th>mean<th>median<th>var<th>mean<th>median<th>var<th>mean<th>...<th>var<th>mean<th>median<th>var<th>mean<th>median<th>var<th>mean<th>median<th>var<tr><th>k<th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><th><tbody><tr><th>0<td>1.311927<td>1<td>3.420319<td>11.448589<td>12.0<td>7.834927<td>5.376147<td>4.0<td>13.922018<td>5.379681<td>...<td>4.668456e+08<td>0.165138<td>0<td>0.194699<td>0.183486<td>0<td>0.262317<td>0.440367<td>0.0<td>0.693170<tr><th>1<td>1.708185<td>1<td>4.600254<td>11.861974<td>12.0<td>7.343786<td>5.156584<td>4.0<td>17.475394<td>5.476736<td>...<td>7.152335e+08<td>0.231317<td>0<td>0.178444<td>0.188612<td>0<td>0.253584<td>0.441281<td>0.0<td>0.554575</table><p>2 rows × 42 columns</p></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_44_1.png" alt="png" /></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="c1"># drop cluster labels
</span><span class="n">Xy_original</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="422-k-medoids">4.2.2 K-medoids</h3><p>K-medoids is similar to Kmeans, but uses an actual data point “medoid” to form clusters rather than the mean value “centroid” location of data wihtin a cluster. K-medoids also takes <a href="https://stats.stackexchange.com/a/94178/100439">different distance metrics to Kmeans</a>, inlcuding Manhattan and Minkowski suited to mixed dataype datasets such as ours. Whilst this makes K-medoids more robust to outliers and feature scale, this is at the expense of added computation and hence is poor perfoming on large datasets and where $k$ is high.</p><p>K-medoids has several implmentations in Python. PAM (partition-around-medoids) is common and implmented in both <a href="https://pypi.org/project/pyclustering/">pyclustering</a> and <a href="https://scikit-learn-extra.readthedocs.io/en/latest/install.html">scikit-learn-extra</a>. See <a href="https://github.com/annoviko/pyclustering/issues/503">here</a> and <a href="https://link.springer.com/chapter/10.1007/978-3-030-32047-8_16">Schubert, 2018</a> for overview of the algorithm implement in pyclustering. Some more usefull links are given below.</p><ul><li><a href="https://stats.stackexchange.com/a/141208/100439">PAM is a variation of K-medoids</a><li><a href="https://towardsdatascience.com/k-medoids-clustering-on-iris-data-set-1931bf781e05">Self defined PAM k-medoids in python</a>.<li><a href="https://pyclustering.github.io/docs/0.10.1/html/d0/dd3/classpyclustering_1_1cluster_1_1kmedoids_1_1kmedoids.html">k-medoids documentation in pyclustering</a></ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">pyclustering.cluster.kmedoids</span> <span class="kn">import</span> <span class="n">kmedoids</span>
<span class="kn">from</span> <span class="n">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">pdist</span><span class="p">,</span><span class="n">squareform</span>
</pre></table></code></div></div><p>K-medoids can be caluclated using many distance metrics. Here the Manhattan or “cityblock” distance is used as this provides a suitable measure where there are both categorical and numerical features. Manhattan distances are computed on the transformed and scaled data set as <code class="language-plaintext highlighter-rouge">scipy.pdist</code> does not perform feature transformation itself.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">mhv</span> <span class="o">=</span> <span class="nf">pdist</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">,</span> <span class="sh">'</span><span class="s">cityblock</span><span class="sh">'</span><span class="p">,)</span>
<span class="n">mh</span> <span class="o">=</span> <span class="nf">squareform</span><span class="p">(</span><span class="n">mhv</span><span class="p">,</span><span class="n">force</span><span class="o">=</span><span class="sh">'</span><span class="s">tomatrix</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Data shape: </span><span class="si">{</span><span class="n">Xy_original</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s">Matrix shape: </span><span class="si">{</span><span class="n">mh</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">mh</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">])</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Data shape: (10302, 23)
Matrix shape: (10302, 10302)
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>0<th>1<th>2<tbody><tr><th>0<td>0.000000<td>5.960622<td>5.921582<tr><th>1<td>5.960622<td>0.000000<td>6.382310<tr><th>2<td>5.921582<td>6.382310<td>0.000000</table></div><p>Note, in contrast to scikit, pyclustering return clusters in an $n$ length list of lists, where $n=k$ and $list$[$n$][$i$] is the index postion from the input distance matrix. These are converted to flat array format.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre><td class="rouge-code"><pre><span class="c1"># find k clusters
</span><span class="n">results_kmedoids</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>

<span class="n">k_cand</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_cand</span><span class="p">:</span>
    <span class="c1"># initiate k random medoids - sets k clusters
</span>    <span class="n">initial_medoids</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">kmedoids_instance</span> <span class="o">=</span> <span class="nf">kmedoids</span><span class="p">(</span><span class="n">mh</span><span class="p">,</span>
                                 <span class="n">initial_medoids</span><span class="p">,</span>
                                 <span class="n">data_type</span><span class="o">=</span><span class="sh">'</span><span class="s">distance_matrix</span><span class="sh">'</span><span class="p">)</span>    

    <span class="c1"># run cluster analysis and obtain results
</span>    <span class="o">%</span><span class="n">time</span> <span class="n">kmedoids_instance</span><span class="p">.</span><span class="nf">process</span><span class="p">()</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="n">kmedoids_instance</span><span class="p">.</span><span class="nf">get_clusters</span><span class="p">()</span>
    <span class="n">medoids</span> <span class="o">=</span> <span class="n">kmedoids_instance</span><span class="p">.</span><span class="nf">get_medoids</span><span class="p">()</span>

    <span class="c1"># convert cluster output
</span>    <span class="n">cluster_array</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">([(</span><span class="n">x</span><span class="p">,</span><span class="n">e</span><span class="p">)</span> <span class="k">for</span> <span class="n">e</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">clusters</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">i</span> <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">i</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">]).</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="n">values</span>
    
    <span class="c1"># score
</span>    <span class="n">score1</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">mh</span><span class="p">,</span> <span class="n">cluster_array</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">precomputed</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">score2</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">,</span> <span class="n">cluster_array</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">correlation</span><span class="sh">'</span><span class="p">)</span>
    
    <span class="c1"># store
</span>    <span class="n">results_kmedoids</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span><span class="n">cluster_array</span><span class="p">,</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">:</span><span class="n">score1</span><span class="p">,</span><span class="sh">'</span><span class="s">s2</span><span class="sh">'</span><span class="p">:</span><span class="n">score2</span><span class="p">}</span>

<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results_kmedoids</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results_kmedoids</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Minkowski</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results_kmedoids</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s2</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results_kmedoids</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">correlation</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">k_cand</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>CPU times: user 1min 7s, sys: 1.7 s, total: 1min 8s
Wall time: 1min 8s
CPU times: user 1min 45s, sys: 3.04 s, total: 1min 48s
Wall time: 1min 48s
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_51_1.png" alt="png" /></p><h3 id="423-clarans">4.2.3 CLARANS</h3><p>CLARANS stands for Clustering Large Applications based on RANdomized Search.There is a good write up of <a href="https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4">CLARANS</a> here. Briefly, CLARANS builds upon the k-medoid and CLARA methods. The key difference in CLARANS is that cluster centers “centroids” or “medoids” are defined through a limited number of rounds of random sampling and of a limited number of neighbourig points. Crucially, the sampling is not limited to just the neighbouring data of a centroid, but may include any point within the entire dataset. Thus avoiding the development of local minima as in CLARA. The CLARANS paper details this and other methods such as k-medoids so is worth a read <a href="http://www.cs.ecu.edu/dingq/CSCI6905/readings/CLARANS.pdf">Raymond 2002</a>.</p><p>The <a href="https://pyclustering.github.io/docs/0.10.1/html/d9/d30/namespacepyclustering_1_1cluster_1_1clarans.html">CLARANS</a> implementation in <a href="https://pyclustering.github.io/">pyclustering</a> requires list of lists as its input dataset. Thus we convert the transformed and scaled features data from numpy array to a list. The other inputs are as follows:</p><ul><li>Input data that is presented as list of points (objects), each point should be represented by list or tuple.<li>number_clusters: amount of clusters that should be allocated.<li>numlocal: the number of local minima obtained (amount of iterations for solving the problem).<li>maxneighbor: the maximum number of neighbors examined. The higher the value of maxneighbor, the closer is CLARANS to K-Medoids, and the longer is each search of a local minima.</ul><p>Note that <a href="https://pyclustering.github.io/docs/0.10.1/html/de/d9f/clarans_8py_source.html">clarans.py</a> uses Euclidean distance metric. There is no functional reason limiting the CLARANS algortihm to use Euclidean distance and below I have made a custom class locally <code class="language-plaintext highlighter-rouge">cclarans</code> using Manhattan distance.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="c1"># customised clarans using Manhattan distance
</span><span class="kn">from</span> <span class="n">cclarans</span> <span class="kn">import</span> <span class="n">cclarans</span>
<span class="kn">from</span> <span class="n">pyclustering.utils</span> <span class="kn">import</span> <span class="n">manhattan_distance</span><span class="p">,</span><span class="n">Euclidean_distance_square</span>

<span class="c1"># manhattan_distance([1,1],[1,2]),Euclidean_distance_square([1,1],[1,2])
# manhattan_distance(Xy_scaled_list[0],Xy_scaled_list[1])
</span>
<span class="n">Xy_scaled_list</span> <span class="o">=</span> <span class="n">Xy_scaled</span><span class="p">.</span><span class="nf">to_numpy</span><span class="p">().</span><span class="nf">tolist</span><span class="p">()</span>
<span class="nf">len</span><span class="p">(</span><span class="n">Xy_scaled_list</span><span class="p">)</span>

<span class="n">clarans_instance</span> <span class="o">=</span> <span class="nf">cclarans</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">Xy_scaled_list</span><span class="p">,</span>
                            <span class="n">number_clusters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                            <span class="n">numlocal</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                            <span class="n">maxneighbor</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># %time clarans_instance.process()
# clusters = clarans_instance.get_clusters()
</span></pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>CPU times: user 18h 52min 28s, sys: 9.66 s, total: 18h 52min 38s
Wall time: 18h 52min 35s
</pre></table></code></div></div><p><strong>OK!</strong> As for K-medoids something is clearly not right here. Our supposedly efficient CLARANS is grinding through taking 18 hours to compelete 1 iteration of candidate $k$.</p><p>To confirm that this is not an implementation issue, below is a run of CLARANS on synthetic data of equal size. And which takes less than 60 seconds to run.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">pyclustering.cluster.clarans</span> <span class="kn">import</span> <span class="n">clarans</span>
<span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>

<span class="c1"># synthetic clusters
</span><span class="n">I</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="nf">make_blobs</span><span class="p">(</span><span class="mi">10302</span><span class="p">,</span><span class="n">n_features</span><span class="o">=</span><span class="mi">36</span><span class="p">,</span><span class="n">centers</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># implement clarans
</span><span class="n">clarans_instance</span> <span class="o">=</span> <span class="nf">clarans</span><span class="p">(</span><span class="n">I</span><span class="p">.</span><span class="nf">tolist</span><span class="p">(),</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>
<span class="o">%</span><span class="n">time</span> <span class="n">clarans_instance</span><span class="p">.</span><span class="nf">process</span><span class="p">()</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="n">clarans_instance</span><span class="p">.</span><span class="nf">get_clusters</span><span class="p">()</span>
<span class="n">medoids</span> <span class="o">=</span> <span class="n">clarans_instance</span><span class="p">.</span><span class="nf">get_medoids</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span><span class="n">axs</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">I</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span><span class="n">I</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Synthetic clusters</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">CLARANS clusters</span><span class="sh">'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">c_</span><span class="p">,</span><span class="n">m</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">clusters</span><span class="p">,</span><span class="n">medoids</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">c_</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">I</span><span class="p">[</span><span class="n">c_</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">scatter</span><span class="p">(</span><span class="n">I</span><span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">I</span><span class="p">[</span><span class="n">m</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span><span class="n">marker</span><span class="o">=</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">black</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">medoids</span><span class="sh">'</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>CPU times: user 38.2 s, sys: 17.6 ms, total: 38.3 s
Wall time: 38.2 s
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_56_1.png" alt="png" /></p><p>As illustrated above, K-means, K-medoids, and CLARANS, are all suited to flat geometry. That is they are suitable for clusters that form convex hulls. Non-flat geometrical clusters include classical shapes such as spirals, crescents, and concentric rings.</p><p>Given the issues and shortcomings of these alogrithms, and Euclidean distance in high demnsional cases, we will now apply alternative distance metrics and clustering algorithms suited to high dimensional non-flat geometry. This includes hierachal, <a href="https://scikit-learn.org/stable/modules/clustering.html#spectral-clustering">spectral</a>, or density based approaches such as <a href="https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html">HDBSCAN</a>.</p><h3 id="424-agglomerative-hierarchical-clustering-with-gower">4.2.4 Agglomerative Hierarchical Clustering with Gower</h3><p><a href="https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html">Agglomerative Clustering</a> is a hierarchical clustering algorithm. In agglomerative clustering, each data point is initially considered as a single cluster, which are then iteratively merged (agglomerated) until all data points form one large cluster. The optimal number of clusters $k$ is found by minimising intra-cluster data point distances and maximising the distance between each unique cluster,</p><p>As outlined in section 3.3.1, the Gower distance is a hybrid metric suited to datasets with mixed datatypes. Gower is implemented here in <a href="https://www.thinkdatascience.com/post/2019-12-16-introducing-python-package-gower/">Gower Python</a>. For R users there is <a href="https://rdrr.io/cran/gower/api">Gower R</a>.</p><p>First, lets take a quick look at the <code class="language-plaintext highlighter-rouge">gower</code> package.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">gower</span>
</pre></table></code></div></div><p>Gower has two core functions, <code class="language-plaintext highlighter-rouge">gower_topn</code> and <code class="language-plaintext highlighter-rouge">gower_matrix</code>. Handily, as per the <a href="https://github.com/wwwjk366/gower/blob/master/gower/gower_dist.py#L61">source code</a> gower also comes with the functionality to handle feature normalisation and scaling so we could pass either our raw or transformed and scaled data.</p><p>The <code class="language-plaintext highlighter-rouge">gower_topn</code> can be used to find $n$ items related to a single row of data. THs could be usefull for quickly finding similar customers customers or for making movie reccomendations for instance. Below I choose a random row of data and find the tp 5 similar items.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">cat</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">categorical_features</span> <span class="k">else</span> <span class="bp">False</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Xy_original</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">focus</span> <span class="o">=</span> <span class="n">Xy_original</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">similar</span> <span class="o">=</span> <span class="n">gower</span><span class="p">.</span><span class="nf">gower_topn</span><span class="p">(</span><span class="n">focus</span><span class="p">,</span><span class="n">Xy_original</span><span class="p">,</span>
                           <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span><span class="n">cat_features</span><span class="o">=</span><span class="n">cat</span><span class="p">)</span>
</pre></table></code></div></div><p>Gower returns the index locations and similarity <code class="language-plaintext highlighter-rouge">values</code>.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="n">similar</span><span class="p">[</span><span class="sh">'</span><span class="s">values</span><span class="sh">'</span><span class="p">])</span>
<span class="n">Xy_original</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">similar</span><span class="p">[</span><span class="sh">'</span><span class="s">index</span><span class="sh">'</span><span class="p">]]</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>[0.         0.08845895 0.09403952 0.10647902 0.11503713]
</pre></table></code></div></div><p>For agglomerative clustering <code class="language-plaintext highlighter-rouge">gower_matrix</code> is used to calculate a similarity matrix between all records in the dataset.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="c1"># create list of cat_feature indicator
</span><span class="n">cat</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">categorical_features</span> <span class="k">else</span> <span class="bp">False</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">Xy_original</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>

<span class="o">%</span><span class="n">time</span> <span class="n">gd</span> <span class="o">=</span> <span class="n">gower</span><span class="p">.</span><span class="nf">gower_matrix</span><span class="p">(</span><span class="n">Xy_original</span><span class="p">,</span> <span class="n">cat_features</span><span class="o">=</span><span class="n">cat</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>CPU times: user 2min 45s, sys: 530 ms, total: 2min 45s
Wall time: 2min 45s
</pre></table></code></div></div><p>Calculating the entire matrix may take some time. Once complete Gower returns the full distance matrix.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Data shape: </span><span class="si">{</span><span class="n">Xy_original</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s">Matrix shape: </span><span class="si">{</span><span class="n">gd</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">gd</span><span class="p">[:</span><span class="mi">3</span><span class="p">,:</span><span class="mi">3</span><span class="p">])</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Data shape: (10302, 24)
Matrix shape: (10302, 10302)
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>0<th>1<th>2<tbody><tr><th>0<td>0.000<td>0.206<td>0.177<tr><th>1<td>0.206<td>0.000<td>0.210<tr><th>2<td>0.177<td>0.210<td>0.000</table></div><p>The <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html#scipy.cluster.hierarchy.linkage">scipy.linkage</a> function is used to perform the hierachal agglomerative clusering. There are a few important considerations here.</p><ul><li><code class="language-plaintext highlighter-rouge">linkage</code> requires a 1D condensed matix as input. We can use <code class="language-plaintext highlighter-rouge">np.squareform</code> to tranform the gower matrix.<li>Some <code class="language-plaintext highlighter-rouge">linkage</code> methods may only be applied to Euclidean distances. <em>“Methods ‘centroid’, ‘median’, and ‘ward’ are correctly defined only if Euclidean pairwise metric is used.”</em> see the documentation for detail.</ul><p>Returned is a ($n-1$) by 4 matrix $Z$. At the $i$-th iteration, clusters with indices $Z[i, 0]$ and $Z[i, 1]$ are combined to form a cluster.The distance between clusters $Z[i, 0]$ and $Z[i, 1]$ is given by $Z[i, 2]$. The fourth value $Z[i, 3]$ represents the number of original observations in the newly formed cluster. The format of $Z$ may be a little confusing at first but these stackoverflow answers (<a href="https://stackoverflow.com/a/37712929/4538066">1</a>,<a href="https://stackoverflow.com/questions/9838861/scipy-linkage-format">2</a>) give a nice explanation.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">scipy.cluster.hierarchy</span> <span class="kn">import</span> <span class="n">linkage</span><span class="p">,</span> <span class="n">fcluster</span><span class="p">,</span> <span class="n">dendrogram</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="c1"># condensed matrix
</span><span class="n">gdv</span> <span class="o">=</span> <span class="nf">squareform</span><span class="p">(</span><span class="n">gd</span><span class="p">,</span><span class="n">force</span><span class="o">=</span><span class="sh">'</span><span class="s">tovector</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># output matrix has format [idx1, idx2, dist, sample_count]
</span><span class="n">Z</span> <span class="o">=</span> <span class="nf">linkage</span><span class="p">(</span><span class="n">gdv</span><span class="p">,</span><span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">complete</span><span class="sh">'</span><span class="p">)</span>
<span class="n">Z_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">id1</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">id2</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">dist</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">n</span><span class="sh">'</span><span class="p">])</span>
<span class="n">Z_df</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>id1<th>id2<th>dist<th>n<tbody><tr><th>0<td>6036.0<td>9801.0<td>0.000<td>2.0<tr><th>1<td>2867.0<td>8191.0<td>0.000<td>2.0</table></div><p>Visualise the hierachal clusters using a <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html#scipy.cluster.hierarchy.dendrogram">scipy.dendogram</a>. Each level and leaf in the dendogram represents a cluster. Clearly displaying the hierachal nature of agglomerative clustering.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dn</span> <span class="o">=</span> <span class="nf">dendrogram</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">truncate_mode</span><span class="o">=</span><span class="sh">'</span><span class="s">level</span><span class="sh">'</span><span class="p">,</span><span class="n">p</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">show_leaf_counts</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">);</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Leaves = </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">dn</span><span class="p">[</span><span class="sh">'</span><span class="s">leaves</span><span class="sh">'</span><span class="p">])</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Leaves = 64
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_72_1.png" alt="png" /></p><p>To find the optimal number of clusters we can again apply silhouette scoring. First, <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html#scipy.cluster.hierarchy.fcluster">scipy.fcluster</a> is applied to to form flat clusters from the hierarchical clustering linkage matrix ($Z$).</p><p>Silhoette scores are calculated using:</p><ul><li>the precomputed Gower distances in the full gower matrix.<li>using correlation</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="c1"># find k clusters
</span><span class="n">results</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">()</span>
<span class="n">k_cand</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">55</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">k_cand</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">500</span><span class="p">,</span><span class="mi">50</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_cand</span><span class="p">:</span>
    <span class="n">cluster_array</span> <span class="o">=</span> <span class="nf">fcluster</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="sh">'</span><span class="s">maxclust</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">score0</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">gd</span><span class="p">,</span> <span class="n">cluster_array</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">precomputed</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">score1</span> <span class="o">=</span> <span class="nf">silhouette_score</span><span class="p">(</span><span class="n">Xy_scaled</span><span class="p">,</span> <span class="n">cluster_array</span><span class="p">,</span><span class="n">metric</span><span class="o">=</span><span class="sh">'</span><span class="s">cityblock</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">:</span><span class="n">cluster_array</span><span class="p">,</span><span class="sh">'</span><span class="s">s0</span><span class="sh">'</span><span class="p">:</span><span class="n">score0</span><span class="p">,</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">:</span><span class="n">score1</span><span class="p">}</span>
    
<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s0</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Gower</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">keys</span><span class="p">()],[</span><span class="n">i</span><span class="p">[</span><span class="sh">'</span><span class="s">s1</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">results</span><span class="p">.</span><span class="nf">values</span><span class="p">()],</span><span class="sh">'</span><span class="s">o-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Cityblock</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">451</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xticks</span><span class="p">(</span><span class="n">k_cand</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_74_0.png" alt="png" /></p><p>Visualising the silhouette scores and plots shows an interesting result. This new round of clusters appears to have an optimal $k$ of around 5, 10, and 35. As happened previously using Kmeans, silhouette scores are shown to improve once $k$ exceeed 100. However, the correlation scores are flat and this is again indicative of small unique groups.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">9</span><span class="p">),</span><span class="n">sharex</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">Xy_original</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="mi">5</span><span class="p">][</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">]</span>

<span class="n">Xy_original</span><span class="p">[</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">sort_index</span><span class="p">().</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">Xy_original</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">column</span><span class="o">=</span><span class="sh">'</span><span class="s">AGE</span><span class="sh">'</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">Xy_original</span><span class="p">.</span><span class="nf">boxplot</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="sh">'</span><span class="s">k</span><span class="sh">'</span><span class="p">,</span><span class="n">column</span><span class="o">=</span><span class="sh">'</span><span class="s">INCOME</span><span class="sh">'</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>&lt;AxesSubplot:title={'center':'INCOME'}, xlabel='k'&gt;
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-03-25-clustering//output_76_1.png" alt="png" /></p><h1 id="5-conclusion">5. Conclusion</h1><p>This post has provided an overview of the key considerations for clustering high dimensional data and with varied datatypes. From feature preparation and transformation, distance metrics, to different clustering techniques and alogirithms.</p><p>Common issues and challenges include: selecting appropriate distance metrics that suit the nature of the data, avoiding sparsity and dimensional issues; computational efficiency of clustering algorithms, and selecting appropriate clustering algorithms given the apriori unknown geometry of the clustrs. Whilst not exhaustive, this post has shown how to approach these challenges using a variety of techniques.</p><p>Additional methods one might also try include: decomposition, applying principal component and multiple correspondence analysis to the data to reduce dimensionality; in addition to other clustering algorithms such as DBSCAN.</p><h1 id="references">References</h1><hr /><ul><li>https://pypi.org/project/gower/<li>https://medium.com/analytics-vidhya/gowers-distance-899f9c4bd553<li>https://towardsdatascience.com/clustering-on-mixed-type-data-8bbd0a2569c3<li>https://medium.com/@rumman1988/clustering-categorical-and-numerical-datatype-using-gower-distance-ab89b3aa90d9<li>https://towardsdatascience.com/hierarchical-clustering-on-categorical-data-in-r-a27e578f2995<li>https://www.researchgate.net/post/What_is_the_best_way_for_cluster_analysis_when_you_have_mixed_type_of_data_categorical_and_scale<li>https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog-post/'>blog-post</a>, <a href='/categories/data-analysis/'>data-analysis</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/clustering/" class="post-tag no-text-decoration" >clustering</a> <a href="/tags/data-mining/" class="post-tag no-text-decoration" >data-mining</a> <a href="/tags/dimension-reduction/" class="post-tag no-text-decoration" >dimension-reduction</a> <a href="/tags/decomposition/" class="post-tag no-text-decoration" >decomposition</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=A guide to clustering large datasets with mixed data-types [updated] - Ben Postance&url=https://bpostance.github.io/posts/clustering-mixed-data/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=A guide to clustering large datasets with mixed data-types [updated] - Ben Postance&u=https://bpostance.github.io/posts/clustering-mixed-data/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=A guide to clustering large datasets with mixed data-types [updated] - Ben Postance&url=https://bpostance.github.io/posts/clustering-mixed-data/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-ai-enterprise/">How to build an Advanced Analytics function</a><li><a href="/posts/multi-tasking-in-python/">Multi-tasking in Python</a><li><a href="/posts/airflow-taskflow/">Writing Pythonic Airflow DAGs with the TaskFlow API</a><li><a href="/posts/finCEN/">The finCEN files: Uncovering money laundering patterns in the global banking network</a><li><a href="/posts/working-with-MODIS-data/">Geospatial Analysis: Working with MODIS data</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/bayes-theory/">bayes-theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian-inference</a> <a class="post-tag" href="/tags/data-mining/">data-mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data-cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension-reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial-analysis</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/clustering-mixed-type-data/"><div class="card-body"> <span class="timeago small" > Feb 20, 2020 <i class="unloaded">2020-02-20T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>A guide to clustering large datasets with mixed data-types</h3><div class="text-muted small"><p> Please see the updated version of this post here</p></div></div></a></div><div class="card"> <a href="/posts/introduction-to-hashing/"><div class="card-body"> <span class="timeago small" > Jun 10, 2019 <i class="unloaded">2019-06-10T21:00:01+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>An introduction to hashing functions for data mining</h3><div class="text-muted small"><p> This post is intended to provide a quick introduction to hash functions and to discuss some practical applications of hashes in data mining and machine learning. The aim of this post is to provide ...</p></div></div></a></div><div class="card"> <a href="/posts/finCEN/"><div class="card-body"> <span class="timeago small" > Sep 21, 2020 <i class="unloaded">2020-09-21T19:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>The finCEN files: Uncovering money laundering patterns in the global banking network</h3><div class="text-muted small"><p> Jupyter notebook here The Financial Crimes Enforcement Network (finCEN) files are more than 2,500 documents, most of which were suspicious activity reports (SARs) files that banks sent to the US ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/pymc3-predictions/" class="btn btn-outline-primary" prompt="Older"><p>Bayesian Inference with PyMC3: pt 2 making predictions</p></a> <a href="/posts/processing-large-spatial-datasets.md/" class="btn btn-outline-primary" prompt="Newer"><p>Geospatial Analysis: obtaining and pre-processing OpenSource satellite data</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//https-bpostance-github-io.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'A guide to clustering large datasets with mixed data-types [updated]'; this.page.url = 'https://bpostance.github.io/posts/clustering-mixed-data/'; this.page.identifier = '/posts/clustering-mixed-data/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/username">BenPostance</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."> Some rights reserved. </span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/bayes-theory/">bayes theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/data-mining/">data mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial analysis</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://bpostance.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
