<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Bayesian Inference with PyMC3: pt 1 posterior distributions" /><meta name="author" content="Ben Postance" /><meta property="og:locale" content="en_US" /><meta name="description" content="Risk, Data Science and Machine Learning" /><meta property="og:description" content="Risk, Data Science and Machine Learning" /><link rel="canonical" href="https://bpostance.github.io/posts/bayesian-inference-pymc3/" /><meta property="og:url" content="https://bpostance.github.io/posts/bayesian-inference-pymc3/" /><meta property="og:site_name" content="Ben Postance" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-15T18:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Bayesian Inference with PyMC3: pt 1 posterior distributions" /><meta name="twitter:site" content="@benpostance" /><meta name="twitter:creator" content="@Ben Postance" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Postance"},"dateModified":"2021-04-19T20:54:48+01:00","datePublished":"2021-01-15T18:00:00+00:00","description":"Risk, Data Science and Machine Learning","headline":"Bayesian Inference with PyMC3: pt 1 posterior distributions","mainEntityOfPage":{"@type":"WebPage","@id":"https://bpostance.github.io/posts/bayesian-inference-pymc3/"},"url":"https://bpostance.github.io/posts/bayesian-inference-pymc3/"}</script><title>Bayesian Inference with PyMC3: pt 1 posterior distributions | Ben Postance</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> // see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> MathJax = { tex: { inlineMath: [ // start/end delimiter pairs for in-line math ['$','$'], ['\\(','\\)'] ], displayMath: [ // start/end delimiter pairs for display math ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-140894462-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-140894462-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="top"></div><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/7165735?s=460&u=7ca34b8152bbff087bfc860cc87a4c35b4f9cb1b&v=4" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Ben Postance</a></div><div class="site-subtitle font-italic">Risk, Data Science and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/bpostance" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/benpostance" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['benpostance','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Bayesian Inference with PyMC3: pt 1 posterior distributions</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Bayesian Inference with PyMC3: pt 1 posterior distributions</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Ben Postance </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Jan 15, 2021, 6:00 PM +0000" prep="on" > Jan 15, 2021 <i class="unloaded">2021-01-15T18:00:00+00:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Apr 19, 2021, 8:54 PM +0100" prefix="Updated " > Apr 19, 2021 <i class="unloaded">2021-04-19T20:54:48+01:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1808 words">10 min</span></div></div><div class="post-content"><p> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/pymc3.png" alt="drawing" width="500" height="250" class="center" /></p><p><strong><a href="https://github.com/bpostance/dsc.learn/blob/main/statistics/bayesian-inference/02.2.bayesian-inference-pymc3-gamma-poisson.ipynb">Jupyter notebook here</a></strong></p><h3 id="introduction">Introduction</h3><p>Here we use PyMC3 on two Bayesian inference case studies: coin-toss and Insurance Claim occurrence. My last post was an introduction to Baye’s theorem and <a href="https://bpostance.github.io/tutorial/mathematics/2020/12/13/introduction-to-bayesian-inference.html">Bayesian inference by hand</a>. There we looked at a simple coin toss scenario, modelling each step by hand, to conclude that we had a bias coin bias with the posterior probability of landing tails P(Tails|Observed Data) = 0.36.</p><p>This time we will run: i) the same coin-toss scenario again, and ii) an example for Poisson insurance claim counts, using the probabilistic programming package <a href="https://docs.pymc.io/https://docs.pymc.io/">PyMC3</a>. An alternative would be to use the equally popular Stanford “Stan” package and its python wrapper <a href="https://pystan.readthedocs.io/en/latest/index.htmlhttps://pystan.readthedocs.io/en/latest/index.html">PyStan</a>. These packages provide an easy to use and intuitive frameworks for developing complex models of data generative processes. In addition, easy access to <a href="https://twiecki.io/blog/2015/11/10/mcmc-sampling/https://twiecki.io/blog/2015/11/10/mcmc-sampling/">Markov Chain Monte Carlo</a> sampling algorithms.</p><h3 id="method">Method:</h3><p>Recall that our initial approach to Bayesian Inference followed:</p><ol><li>Set prior assumptions and establish “known knowns” of our data based on heuristics, historical, or sample data.<li>Formalise a Mathematical Model of the problem space and prior assumptions.<li>Formalise the Prior Distributions.<li>Apply Baye’s theorem to derive the posterior parameter values from observed sample data.<li>Repeat steps 1-4 as more data samples are obtained.</ol><p>Using PyMC3 we can now simplify and condense these steps down.</p><p><strong><em>First, we set our prior belief and prior beta-binomial distribution.</em></strong></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="c1"># prior estimates
</span><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c1"># prior belief of landing tails given coin is unbias
</span><span class="n">prior</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.01</span><span class="p">,.</span><span class="mi">01</span><span class="p">)</span>

<span class="c1"># beta-binomial prior distribution
</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span>
<span class="n">prior_beta</span>  <span class="o">=</span> <span class="nf">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="n">prior_beta</span> <span class="o">=</span> <span class="n">prior_beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="o">/</span> <span class="n">prior_beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">).</span><span class="nf">sum</span><span class="p">()</span> <span class="c1"># sample integral pmf
</span>
<span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">prior_beta</span><span class="p">,</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Beta Prior</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-01.png" alt="png" /></p><p><strong><em>Second, we define and inspect our sample observation data</em></strong></p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># observation data
</span><span class="n">trials</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">tails</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">heads</span> <span class="o">=</span> <span class="n">trials</span><span class="o">-</span><span class="n">tails</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Trials:</span><span class="se">\t</span><span class="si">{</span><span class="n">trials</span><span class="si">}</span><span class="se">\n</span><span class="s">tails:</span><span class="se">\t</span><span class="si">{</span><span class="n">tails</span><span class="si">}</span><span class="se">\n</span><span class="s">heads:</span><span class="se">\t</span><span class="si">{</span><span class="n">heads</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Observed P(tails) = </span><span class="si">{</span><span class="n">tails</span><span class="o">/</span><span class="n">trials</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>Trials:	50
tails:	15
heads:	35
Observed P(tails) = 0.3
</pre></table></code></div></div><p><strong><em>Third, we define and run our mathematical model</em></strong></p><p>Notice, PyMC3 provides a clean and efficient syntax for describing prior distributions and observational data from which we can include or separately initiate the model sampling. Also notice that PyMC3 allows us to define priors, introduce sample observation data, and initiate posterior simulation in the same cell call.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">pymc3</span> <span class="k">as</span> <span class="n">pm</span>

<span class="k">with</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    
    <span class="c1"># Define the prior beta distribution
</span>    <span class="n">theta_prior</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Beta</span><span class="p">(</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
    
    <span class="c1"># Observed outcomes in the sample dataset.
</span>    <span class="n">observations</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Binomial</span><span class="p">(</span><span class="sh">'</span><span class="s">obs</span><span class="sh">'</span><span class="p">,</span>
                               <span class="n">n</span> <span class="o">=</span> <span class="n">trials</span><span class="p">,</span>
                               <span class="n">p</span> <span class="o">=</span> <span class="n">theta_prior</span><span class="p">,</span>
                               <span class="n">observed</span> <span class="o">=</span> <span class="n">tails</span><span class="p">)</span>
     
    <span class="c1"># NUTS, the No U-Turn Sampler (Hamiltonian)
</span>    <span class="n">step</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">NUTS</span><span class="p">()</span>     
    <span class="c1"># Evaluate draws=n on chains=n 
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span><span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">,</span><span class="n">chains</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">progressbar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  
<span class="n">model</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>Multiprocess sampling (2 chains in 4 jobs)
NUTS: [prior]
Sampling 2 chains, 0 divergences: 100%|██████████| 2000/2000 [00:00&lt;00:00, 2145.48draws/s]
</pre></table></code></div></div>\[\begin{array}{rcl} \text{prior} &amp;\sim &amp; \text{Beta}(\mathit{alpha}=7.0,~\mathit{beta}=7.0)\\\text{obs} &amp;\sim &amp; \text{Binomial}(\mathit{n}=50,~\mathit{p}=\text{prior}) \end{array}\]<h3 id="results">Results</h3><p>Or with more sampling and across more chains. Then the trace summary returns useful summary statistics of model performance:</p><ul><li>mc_error estimates simulation error by breaking the trace into batches, computing the mean of each batch, and then the standard deviation of these means.<li>hpd_* gives highest posterior density intervals. The 2.5 and 97.5 labels are a bit misleading. There are lots of 95% credible intervals, depending on the relative weights of the left and right tails. The 95% HPD interval is the narrowest among these 95% intervals.<li>Rhat is sometimes called the potential scale reduction factor, and gives us a factor by which the variance might be reduced, if our MCMC chains had been longer. It’s computed in terms of the variance between chains vs within each chain. Values near 1 are good.</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">with</span> <span class="n">model</span><span class="p">:</span>    
    <span class="c1"># Evaluate draws=n on chains=n 
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span><span class="n">chains</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">summary</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">summary</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 4 jobs)
NUTS: [prior]
Sampling 3 chains, 0 divergences: 100%|██████████| 6000/6000 [00:01&lt;00:00, 3186.22draws/s]
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>mean<th>sd<th>hpd_3%<th>hpd_97%<th>mcse_mean<th>mcse_sd<th>ess_mean<th>ess_sd<th>ess_bulk<th>ess_tail<th>r_hat<tbody><tr><td>prior<td>0.343<td>0.057<td>0.236<td>0.455<td>0.001<td>0.001<td>1793.0<td>1793.0<td>1775.0<td>2832.0<td>1.0</table></div><p>We use the trace to manually plot and compare the prior and posterior distributions. Confirming these are similar to what was obtained by hand with a posterior distribution mean of P(Tails|Observed Data) = 0.35.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-02.png" alt="png" /></p><p>However, PyMC3 also provides functionality to create trace plots, posterior distribution plot, credible intervals, plus many more (e.g. see <a href="https://docs.pymc.io/api/plots.html">official docs</a> and useful examples <a href="https://rlhick.people.wm.edu/stories/bayesian_7.html">here</a>).</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># note there are a few user warnings on using pymc3.traceplot() with pyplot
</span><span class="kn">import</span> <span class="n">warnings</span>
<span class="k">with</span> <span class="n">warnings</span><span class="p">.</span><span class="nf">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="p">.</span><span class="nf">simplefilter</span><span class="p">(</span><span class="sh">"</span><span class="s">ignore</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pm</span><span class="p">.</span><span class="nf">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    <span class="n">pm</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span><span class="n">ref_val</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-03.png" alt="png" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-04.png" alt="png" /></p><p>And there we have it. PyMC3 and other similar packages offer an easy set of functions to assemble and run probabilistic simulations such as Bayesian Inference. Below I will show one more example for Insurance claim counts.</p><h3 id="case-study">Case Study:</h3><h4 id="evaluating-insurance-claim-occurrences-using-bayesian-inference">Evaluating Insurance claim occurrences using Bayesian Inference</h4><p>Insurance claims are typically modelled as occurring due to a <a href="https://en.wikipedia.org/wiki/Poisson_distributionhttps://en.wikipedia.org/wiki/Poisson_distribution">Poisson distributed</a> process. A discrete probability distribution that gives the expected number of events (e.g. claims) to occurring within a time interval (e.g. per year). The Poisson distribution has many applications such as modelling machine component failure rates, customer arrival rates, website traffic, and storm events. First, we will run this through by hand as before and then using PyMC3.</p><p>The Poisson distribution is given by:</p>\[f(y_i|λ)=\frac{e^{−λ}λ^{y_i}}{y_i!}\]<p>Where lambda λ is the “rate” of events given by the total number of events (k) divided by the number of units (n) in the data (λ = k/n). In the Poisson distribution the expected value E(Y), mean E(X), and variance Var(Y) of Poisson distribution are the same;</p><p>e.g., E(Y) = E(X) = Var(X) = λ.</p><p><strong>Note</strong> that if the variance is greater than the mean, the data is said to be overdispersed. This is common in insurance claim data with lots of zeros and is better handled by the NegativeBinomial and <a href="https://stats.idre.ucla.edu/r/dae/zip/">zero-inflated models</a> such as ZIP and ZINB.</p><p><strong><em>First, establish the prior distribution</em></strong><br /></p><p>Here we generate some observed data that follows a poisson distribution with a rate, lambda, λ = 2.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">lam_</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">poisson</span><span class="p">(</span><span class="n">lam</span><span class="o">=</span><span class="n">lam_</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Mean={:.2f}, Var={:.2f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">y</span><span class="p">),</span><span class="n">np</span><span class="p">.</span><span class="nf">var</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>

<span class="c1"># compute histogram
</span><span class="n">obs_</span><span class="p">,</span><span class="n">bins</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">histogram</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span><span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot
</span><span class="n">fig</span><span class="p">,</span><span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">bins</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span><span class="n">obs_</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Observed</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Histogram: Simulated Poisson $y$</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Poisson lambda=λ</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">P(λ)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Mean=2.04, Var=2.03
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-05.png" alt="png" /></p><p>The observed poisson generated data has a right-skew “heavy tail”. It looks like a gamma distribution. The <a href="http://www.math.wm.edu/~leemis/chart/UDR/PDFs/Gammapoisson.pdf">gamma-poisson model</a> is a poisson distribution where λ lambda is gamma distribution. The gamma-poisson distributions form a <a href="https://vioshyvo.github.io/Bayesian_inference/conjugate-distributions.htmlhttps://vioshyvo.github.io/Bayesian_inference/conjugate-distributions.html">conjugate prior</a>.</p><p>We could use a beta-poisson, or any distribution that resembles the shape the observed lambda data, however gamma-poisson is most suitable as:</p><ul><li>poisson can take on any positive number to infinity(0,∞), whereas a beta or uniform is [0-100].<li>gamma and poisson are in the same distribution family.<li>gamma has a peak close to zero.<li>gamma tail goes to infinity.</ul><p><a href="https://vioshyvo.github.io/Bayesian_inference/conjugate-distributions.html">Hyvönen &amp; Tolonen 2019</a></p><p>The gamma-poisson prior is then:</p>\[λ∼Γ(a,b)\]<p>Where a is the gamma shape and b is the gamma rate parameter. The gamma density functions is:</p>\[f(λ)=\frac{b^a}{Γ(a)}λ^{a−1}e^{−bλ}\]<p>where a&gt;0 is the shape parameter,and b&gt;0 is the rate parameter, and</p>\[E(λ)=\frac{a}{b}\]<p>and</p>\[Var(λ)=\frac{a}{b^2}\]<p><strong><em>Note</em></strong> In scipy the gamma distribution uses the Shape <em>a</em> and Scale parameterisation, where rate <em>b</em> is equal to inverse of scale (rate = 1/scale). See more detail on scipy.gamma in the <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html">docs</a>, and in these SO answers <a href="https://stackoverflow.com/a/2896284/4538066">1</a>,<a href="https://stackoverflow.com/a/29208871/4538066">2</a>,<a href="https://stackoverflow.com/a/29902949/4538066">3</a>.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="c1"># Parameters of the prior gamma distribution.
# shape, loc, scale == alpha,loc,beta
</span><span class="n">a</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">stats</span><span class="p">.</span><span class="n">gamma</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">rate</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">b</span> <span class="c1"># stats.gamma.fit(y)[2] ==  rate = 1/scale
</span><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Shape alpha:{:.2f}</span><span class="se">\n</span><span class="s">Loc:{:.2f}</span><span class="se">\n</span><span class="s">Scale beta:{:.2f}</span><span class="se">\n</span><span class="s">Rate:{:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">loc</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">rate</span><span class="p">))</span>

<span class="c1"># options for lambda
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> 

<span class="c1"># Define the prior distribution.
</span><span class="n">prior</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">stats</span><span class="p">.</span><span class="n">gamma</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">rate</span><span class="p">,</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">priors</span> <span class="o">=</span> <span class="nf">prior</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># plot
</span><span class="n">axs</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Gamma</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">'</span><span class="s">Prior Gamma-poisson density function for a={:.2f} and b={:.2f}</span><span class="sh">'</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">))</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Poisson lambda=λ</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">P(λ)</span><span class="sh">'</span><span class="p">)</span>
<span class="n">axs</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">fig</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>Shape alpha:3.51
Loc:-0.76
Scale beta:0.80
Rate:1.25
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-06.png" alt="png" /></p><p><strong><em>Second, the likelihood function and posterior</em></strong><br /></p><p>The gamma function is often referred to as the generalized factorial since:</p>\[Γ(n+1) = n!\]<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">scipy.special</span> <span class="k">as</span> <span class="n">sp</span>

<span class="c1"># generalised gamma == factorial
</span><span class="n">n</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">sp</span><span class="p">.</span><span class="nf">gamma</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">math</span><span class="p">.</span><span class="nf">factorial</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>True
</pre></table></code></div></div><p>then the likelihood function is:</p>\[f(y|λ)=\prod_{i=1}^{n}\frac{e^{−λ}λ^{λ_i}}{y_i!} = \frac{e^{-nλ}λ\sum_{i=1}^{n}y_i}{\prod_{i=1}^{n}y_i!}\]<p>then as</p>\[f(λ|y)∝λ^(\sum_{i=1}^{n}y_i+a)^{−1}e^{−(n+b)λ}\]<p>the posterior distribution is again a gamma</p>\[f(λ|y)=Γ\big(\sum_{i=1}^{n}y_i+a,n+b\big)\]<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">posterior</span><span class="p">(</span><span class="n">lam</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    
    <span class="n">shape</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">y</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    <span class="n">rate</span> <span class="o">=</span> <span class="n">b</span> <span class="o">+</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span>
    
    <span class="k">return</span> <span class="n">stats</span><span class="p">.</span><span class="n">gamma</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">lam</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">rate</span><span class="p">)</span>

<span class="n">posterior</span> <span class="o">=</span> <span class="nf">posterior</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">priors</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">prior</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">posterior</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">blue</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">posterior</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">();</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-07.png" alt="png" /></p><p>As shown the the posterior mean (blue) is centered around the true lambda rate that we set at the start. The posterior mean is:</p>\[\frac{\sum_{i=1}^{n}y_i+a}{n+b}\]<p>i.e. the posterior mean is a weighted average of the prior mean and the observed sample average</p>\[\bar{y}\]<div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"""</span><span class="s">True lambda: </span><span class="si">{</span><span class="n">lam_</span><span class="si">}</span><span class="s">
prior mean: </span><span class="si">{</span><span class="n">a</span><span class="o">/</span><span class="n">b</span><span class="si">}</span><span class="s">
posterior mean: </span><span class="si">{</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">y</span><span class="p">.</span><span class="nf">sum</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">b</span><span class="o">+</span><span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">)</span><span class="si">}</span><span class="s">
sample mean:</span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">}</span><span class="sh">"""</span><span class="p">)</span>

</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>True lambda: 2
prior mean: 4.406142620913817
posterior mean: 2.0408855867928644
sample mean:2.039
</pre></table></code></div></div><p>Now lets re-produce the steps above in PyMC3.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="c1"># known initial params
</span><span class="nf">print</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">lam_</span><span class="p">,</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>(3.5125841350931166, 0.7972016426387533, 2, (1000,))
</pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">model</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    
    <span class="c1"># Define the prior of the parameter lambda.
</span>    <span class="n">prior_lam</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Gamma</span><span class="p">(</span><span class="sh">'</span><span class="s">prior-gamma-lambda</span><span class="sh">'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">b</span><span class="p">)</span>
    
    <span class="c1"># Define the likelihood function.
</span>    <span class="n">y_obs</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Poisson</span><span class="p">(</span><span class="sh">'</span><span class="s">y_obs</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">prior_lam</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Consider 2000 draws and 3 chains.
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">draws</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">chains</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">summary</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">summary</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 4 jobs)
NUTS: [prior-gamma-lambda]
Sampling 3 chains, 0 divergences: 100%|██████████| 7500/7500 [00:02&lt;00:00, 3101.04draws/s]
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>mean<th>sd<th>hpd_3%<th>hpd_97%<th>mcse_mean<th>mcse_sd<th>ess_mean<th>ess_sd<th>ess_bulk<th>ess_tail<th>r_hat<tbody><tr><td>prior-gamma-lambda<td>2.04<td>0.045<td>1.954<td>2.124<td>0.001<td>0.001<td>2487.0<td>2485.0<td>2483.0<td>3839.0<td>1.0</table></div><p>The above configuration simulates the posterior distribution for 3 independent Markov-Chains (traces). The traceplots display the results for each simulation.</p><p>Below the mean, quantile, credible interval (HPD) 94%, and arbitrary reference value (orange vertical).</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="c1"># note there are a few user warnings on using pymc3.traceplot() with pyplot
</span><span class="kn">import</span> <span class="n">warnings</span>
<span class="k">with</span> <span class="n">warnings</span><span class="p">.</span><span class="nf">catch_warnings</span><span class="p">():</span>
    <span class="n">warnings</span><span class="p">.</span><span class="nf">simplefilter</span><span class="p">(</span><span class="sh">"</span><span class="s">ignore</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">pm</span><span class="p">.</span><span class="nf">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
    <span class="n">pm</span><span class="p">.</span><span class="nf">plot_posterior</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span><span class="n">ref_val</span><span class="o">=</span><span class="mf">2.5</span><span class="p">);</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-08.png" alt="png" /></p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-01-15-bayesian-inference-pymc3-09.png" alt="png" /></p><p>You may have noticed that in this example that we have defined our prior distribution from the observed data and applied bayesian inference on this data to derive the posterior distribution, confirming a lambda of 2.</p><h3 id="conclusion">Conclusion:</h3><p>In this post PyMC3 was applied to perform Bayesian Inference on two examples: coin toss bias using the beta-binomial distribution, and insurance claim occurrence using the gamma-poisson distribution.</p><h3 id="references">References</h3><hr /><ul><li>https://twiecki.io/blog/2015/11/10/mcmc-sampling/https://twiecki.io/blog/2015/11/10/mcmc-sampling/<li>https://docs.pymc.io/https://docs.pymc.io/<li>https://pystan.readthedocs.io/en/latest/index.htmlhttps://pystan.readthedocs.io/en/latest/index.html<li>https://www.sciencedirect.com/science/article/abs/pii/S0740002005000249<li>https://vioshyvo.github.io/Bayesian_inference/conjugate-distributions.html<li>https://www.coursera.org/learn/mcmc-bayesian-statistics</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog-post/'>blog-post</a>, <a href='/categories/data-analysis/'>data-analysis</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/bayesian-inference/" class="post-tag no-text-decoration" >bayesian-inference</a> <a href="/tags/bayes-theory/" class="post-tag no-text-decoration" >bayes-theory</a> <a href="/tags/pymc3/" class="post-tag no-text-decoration" >pymc3</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Bayesian Inference with PyMC3: pt 1 posterior distributions - Ben Postance&url=https://bpostance.github.io/posts/bayesian-inference-pymc3/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Bayesian Inference with PyMC3: pt 1 posterior distributions - Ben Postance&u=https://bpostance.github.io/posts/bayesian-inference-pymc3/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Bayesian Inference with PyMC3: pt 1 posterior distributions - Ben Postance&url=https://bpostance.github.io/posts/bayesian-inference-pymc3/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-ai-enterprise/">How to build an Advanced Analytics function</a><li><a href="/posts/multi-tasking-in-python/">Multi-tasking in Python</a><li><a href="/posts/airflow-taskflow/">Writing Pythonic Airflow DAGs with the TaskFlow API</a><li><a href="/posts/finCEN/">The finCEN files: Uncovering money laundering patterns in the global banking network</a><li><a href="/posts/working-with-MODIS-data/">Geospatial Analysis: Working with MODIS data</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/bayes-theory/">bayes-theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian-inference</a> <a class="post-tag" href="/tags/data-mining/">data-mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data-cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension-reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial-analysis</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/pymc3-predictions/"><div class="card-body"> <span class="timeago small" > Feb 20, 2021 <i class="unloaded">2021-02-20T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bayesian Inference with PyMC3: pt 2 making predictions</h3><div class="text-muted small"><p> Jupyter notebook here In this post I will show how Bayesian inference is applied to train a model and make predictions on out-of-sample test data. For this, we will build two models using a case s...</p></div></div></a></div><div class="card"> <a href="/posts/introduction-to-bayesian-inference/"><div class="card-body"> <span class="timeago small" > Dec 13, 2020 <i class="unloaded">2020-12-13T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bayesian Inference by hand</h3><div class="text-muted small"><p> image source Jupyter notebook here Bayesian inference is a statistical method used to update one’s beliefs about a process or system upon observing data. It has wide reaching applications from...</p></div></div></a></div><div class="card"> <a href="/posts/finCEN/"><div class="card-body"> <span class="timeago small" > Sep 21, 2020 <i class="unloaded">2020-09-21T19:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>The finCEN files: Uncovering money laundering patterns in the global banking network</h3><div class="text-muted small"><p> Jupyter notebook here The Financial Crimes Enforcement Network (finCEN) files are more than 2,500 documents, most of which were suspicious activity reports (SARs) files that banks sent to the US ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/introduction-to-bayesian-inference/" class="btn btn-outline-primary" prompt="Older"><p>Bayesian Inference by hand</p></a> <a href="/posts/pymc3-predictions/" class="btn btn-outline-primary" prompt="Newer"><p>Bayesian Inference with PyMC3: pt 2 making predictions</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//https-bpostance-github-io.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'Bayesian Inference with PyMC3: pt 1 posterior distributions'; this.page.url = 'https://bpostance.github.io/posts/bayesian-inference-pymc3/'; this.page.identifier = '/posts/bayesian-inference-pymc3/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/username">BenPostance</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."> Some rights reserved. </span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/bayes-theory/">bayes theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/data-mining/">data mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial analysis</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://bpostance.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
