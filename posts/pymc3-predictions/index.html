<!DOCTYPE html><html lang="en-US" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-cache-enabled" content="false"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Bayesian Inference with PyMC3: pt 2 making predictions" /><meta name="author" content="Ben Postance" /><meta property="og:locale" content="en_US" /><meta name="description" content="Jupyter notebook here" /><meta property="og:description" content="Jupyter notebook here" /><link rel="canonical" href="https://bpostance.github.io/posts/pymc3-predictions/" /><meta property="og:url" content="https://bpostance.github.io/posts/pymc3-predictions/" /><meta property="og:site_name" content="Ben Postance" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-02-20T18:00:00+00:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Bayesian Inference with PyMC3: pt 2 making predictions" /><meta name="twitter:site" content="@benpostance" /><meta name="twitter:creator" content="@Ben Postance" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ben Postance"},"dateModified":"2021-04-19T20:54:48+01:00","datePublished":"2021-02-20T18:00:00+00:00","description":"Jupyter notebook here","headline":"Bayesian Inference with PyMC3: pt 2 making predictions","mainEntityOfPage":{"@type":"WebPage","@id":"https://bpostance.github.io/posts/pymc3-predictions/"},"url":"https://bpostance.github.io/posts/pymc3-predictions/"}</script><title>Bayesian Inference with PyMC3: pt 2 making predictions | Ben Postance</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> // see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> MathJax = { tex: { inlineMath: [ // start/end delimiter pairs for in-line math ['$','$'], ['\\(','\\)'] ], displayMath: [ // start/end delimiter pairs for display math ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-140894462-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-140894462-1'); }); </script><body data-spy="scroll" data-target="#toc"><div id="top"></div><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://avatars.githubusercontent.com/u/7165735?s=460&u=7ca34b8152bbff087bfc860cc87a4c35b4f9cb1b&v=4" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Ben Postance</a></div><div class="site-subtitle font-italic">Risk, Data Science and Machine Learning</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT ME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/bpostance" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/benpostance" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['benpostance','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Bayesian Inference with PyMC3: pt 2 making predictions</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Bayesian Inference with PyMC3: pt 2 making predictions</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Ben Postance </span> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Feb 20, 2021, 6:00 PM +0000" prep="on" > Feb 20, 2021 <i class="unloaded">2021-02-20T18:00:00+00:00</i> </span></div><div> <span> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Apr 19, 2021, 8:54 PM +0100" prefix="Updated " > Apr 19, 2021 <i class="unloaded">2021-04-19T20:54:48+01:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2236 words">12 min</span></div></div><div class="post-content"><p><strong><a href="https://github.com/bpostance/dsc.learn/blob/main/statistics/bayesian-inference/03.1-bayesian-regression-oos-predictions.ipynb">Jupyter notebook here</a></strong></p><p>In this post I will show how Bayesian inference is applied to train a model and make predictions on out-of-sample test data. For this, we will build two models using a case study of predicting student grades on a classical dataset. The first model is a classic frequentist normally distributed regression General Linear Model (GLM). While the second is, again A normal GLM, but built using the Bayesian inference method.</p><h1 id="case-study-predicting-student-grades">Case Study: predicting student grades</h1><p>The objective is to develop a model that can predict student grades given several input factors about each student. The publicly available <a href="https://archive.ics.uci.edu/ml/datasets/student+performance#">UCI dataset</a> contains grades and factors for 649 students taking a Portuguese language course. #</p><p><strong><em>Dependent variable or “Target”</em></strong></p><ul><li>“G3” is the student’s final grade for Portuguese (numeric: from 0 to 20, output target)</ul><p><strong><em>Independent variables or “Features”</em></strong></p><p>A subset of numeric and categorical features is used to build the initial model:</p><ul><li>“age” student age from 15 to 22<li>“internet” student has internet access at home (binary: yes or no)<li>“failures” is the number of past class failures (cat: n if 1&lt;=n&lt;3, else 4)<li>“higher” wants to take higher education (binary: yes or no)<li>“Medu” mother’s education (cat: 0 - none, 1 - primary education (4th grade), 2 5th to 9th grade, 3 secondary education or 4 higher education)<li>"”Fedu father’s education (cat: 0 - none, 1 - primary education (4th grade), 2 5th to 9th grade, 3 secondary education or 4 higher education)<li>“studytime” weekly study time (cat: 1 - &lt;2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - &gt;10 hours)<li>“absences” number of school absences (numeric: from 0 to 93)</ul><p>The final dataset looks like this and here is the distribution of acheived student grades for Portuguese.</p><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>internet<th>higher<th>age<th>absences<th>failures<th>Medu<th>Fedu<th>studytime<th>G3<tbody><tr><th>0<td>0<td>1<td>18<td>4<td>0<td>4<td>4<td>2<td>11</table></div><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">sns</span><span class="p">.</span><span class="nf">displot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="n">x</span><span class="o">=</span><span class="sh">'</span><span class="s">G3</span><span class="sh">'</span><span class="p">,</span><span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">);</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_6_1.png" alt="png" /></p><h1 id="glm-frequentist-model">GLM: frequentist model</h1><p>First, I will build a regular GLM using <code class="language-plaintext highlighter-rouge">statsmodels</code> to illustrate the frequentist approach.</p><p>There are two binary categorical features for <em>Internet</em> usage and <em>Higher</em> education. <em>Age</em> is a numerical feature. The remaining features are numerical values that have been cut into ordinal categories. Practitioners may treat these differently depending on the model objective and nature of the feature. Here I will treat them as numerical within range <code class="language-plaintext highlighter-rouge">0 to max(feature)</code> range. In statsmodels we can use patsy design matrices and formula to specify how we want to treat each variable. For instance, for categoricals, we can use <code class="language-plaintext highlighter-rouge">C(Internet, Treatment(0))</code> to encode Internet as a categorical variable with the reference level set to (0).</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1"># model grade data
</span><span class="n">formula</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span> <span class="k">if</span> <span class="n">f</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">categoricals</span> <span class="k">else</span> <span class="sa">f</span><span class="sh">"</span><span class="s">C(</span><span class="si">{</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">features</span><span class="p">]</span>
<span class="n">formula</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s"> ~ </span><span class="sh">'</span> <span class="o">+</span> <span class="sh">'</span><span class="s"> + </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">formula</span><span class="p">)</span>
<span class="n">glm_</span> <span class="o">=</span> <span class="n">smf</span><span class="p">.</span><span class="nf">glm</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span>
              <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span>
              <span class="n">family</span><span class="o">=</span><span class="n">sm</span><span class="p">.</span><span class="n">families</span><span class="p">.</span><span class="nc">Gaussian</span><span class="p">())</span>
<span class="n">glm</span> <span class="o">=</span> <span class="n">glm_</span><span class="p">.</span><span class="nf">fit</span><span class="p">()</span>
<span class="n">glm</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</pre></table></code></div></div><table class="simpletable"><caption>Generalized Linear Model Regression Results<tr><th>Dep. Variable:<td>G3<th> No. Observations:<td> 634<tr><th>Model:<td>GLM<th> Df Residuals:<td> 625<tr><th>Model Family:<td>Gaussian<th> Df Model:<td> 8<tr><th>Link Function:<td>identity<th> Scale:<td> 5.1895<tr><th>Method:<td>IRLS<th> Log-Likelihood:<td> -1417.1<tr><th>Date:<td>Fri, 12 Mar 2021<th> Deviance:<td> 3243.4<tr><th>Time:<td>13:39:15<th> Pearson chi2:<td>3.24e+03<tr><th>No. Iterations:<td>3<th><td><tr><th>Covariance Type:<td>nonrobust<th><td></table><table class="simpletable"><tr><td><th>coef<th>std err<th>z<th>P&gt;|z|<th>[0.025<th>0.975]<tr><th>Intercept<td> 3.6741<td> 1.430<td> 2.570<td> 0.010<td> 0.872<td> 6.476<tr><th>C(internet)[T.1]<td> 0.2882<td> 0.225<td> 1.282<td> 0.200<td> -0.152<td> 0.729<tr><th>C(higher)[T.1]<td> 1.8413<td> 0.328<td> 5.622<td> 0.000<td> 1.199<td> 2.483<tr><th>age<td> 0.3185<td> 0.081<td> 3.945<td> 0.000<td> 0.160<td> 0.477<tr><th>absences<td> -0.0739<td> 0.020<td> -3.674<td> 0.000<td> -0.113<td> -0.034<tr><th>failures<td> -1.4217<td> 0.170<td> -8.339<td> 0.000<td> -1.756<td> -1.088<tr><th>Medu<td> 0.3847<td> 0.108<td> 3.567<td> 0.000<td> 0.173<td> 0.596<tr><th>Fedu<td> 0.0332<td> 0.108<td> 0.306<td> 0.760<td> -0.179<td> 0.246<tr><th>studytime<td> 0.4307<td> 0.112<td> 3.839<td> 0.000<td> 0.211<td> 0.651</table><p><br /> Some quick observations on the above. Most features have a statistically significant linear relaitonship with grade with the exception of Fathers education. The sign of the regression coefficients also hold with our logic. More absensences and failures is shown to have a negative influence on predicted grade. Whereas studytime and desire to go on to higher education having positive influence on predicted grade.</p><p>Below we see there is an outlier in the data.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_14_0.png" alt="png" /></p><h1 id="pymc3-glm-bayesian-model">PyMC3 GLM: Bayesian model</h1><p>Now let’s re-build our model using PyMC3. As described in this <a href="https://twiecki.io/blog/2013/08/12/bayesian-glms-1/">blog post</a> PyMC3 has its own <code class="language-plaintext highlighter-rouge">glm.from_formula()</code> function that behaves similar to statsmodels. It even accepts the same patsy formula.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">pymc3</span> <span class="k">as</span> <span class="n">pm</span>
<span class="kn">import</span> <span class="n">arviz</span> <span class="k">as</span> <span class="n">avz</span>

<span class="c1"># note we can re-use our formula 
</span><span class="nf">print</span><span class="p">(</span><span class="n">formula</span><span class="p">)</span>

<span class="n">bglm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">bglm</span><span class="p">:</span>
    <span class="c1"># Normally distributed priors
</span>    <span class="n">family</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="n">glm</span><span class="p">.</span><span class="n">families</span><span class="p">.</span><span class="nc">Normal</span><span class="p">()</span>
    <span class="c1"># create the model 
</span>    <span class="n">pm</span><span class="p">.</span><span class="n">GLM</span><span class="p">.</span><span class="nf">from_formula</span><span class="p">(</span><span class="n">formula</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="n">family</span><span class="o">=</span><span class="n">family</span><span class="p">)</span>
    <span class="c1"># sample
</span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>G3 ~ C(internet) + C(higher) + age + absences + failures + Medu + Fedu + studytime

Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (4 chains in 4 jobs)
NUTS: [sd, studytime, Fedu, Medu, failures, absences, age, C(higher)[T.1], C(internet)[T.1], Intercept]
</pre></table></code></div></div><div><style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; }</style><progress value="8000" class="" max="8000" style="width:300px; height:20px; vertical-align: middle;"> </progress> 100.00% [8000/8000 00:27&lt;00:00 Sampling 4 chains, 0 divergences]</div><p><br /> Once the model has run we can examine the model posterior distribution samples. This is akin to viewing the <code class="language-plaintext highlighter-rouge">model.summary()</code> of a regular GLM as above. In this Bayesian model summary table the mean is the coefficient estimate from the posterior distribution. Here we see the posterior distribution of the model intercept is around 4.9. Indicating a student is expected to attain at least a grade of 4.9 irrespective of what we know about them.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">summary</span> <span class="o">=</span> <span class="n">avz</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
<span class="n">summary</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
</pre></table></code></div></div><div><style scoped=""> .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</style><table border="1" class="dataframe"><thead><tr style="text-align: right;"><th><th>mean<th>sd<th>hdi_3%<th>hdi_97%<th>mcse_mean<th>mcse_sd<th>ess_bulk<th>ess_tail<th>r_hat<tbody><tr><th>Intercept<td>3.724<td>1.421<td>0.997<td>6.260<td>0.026<td>0.019<td>2972.0<td>2599.0<td>1.0<tr><th>C(internet)[T.1]<td>0.287<td>0.221<td>-0.126<td>0.706<td>0.003<td>0.003<td>4183.0<td>2580.0<td>1.0<tr><th>C(higher)[T.1]<td>1.836<td>0.332<td>1.188<td>2.450<td>0.005<td>0.004<td>4244.0<td>2697.0<td>1.0<tr><th>age<td>0.315<td>0.080<td>0.167<td>0.463<td>0.001<td>0.001<td>2950.0<td>2281.0<td>1.0<tr><th>absences<td>-0.074<td>0.021<td>-0.114<td>-0.035<td>0.000<td>0.000<td>5348.0<td>2799.0<td>1.0</table></div><p><br /> Rather than p-values we have highest posterior density “hpd”. The 3-97% hpd fields and value range indicates the credible interval for the true value of our parameter. As for classical models if this range crosses 0, from negative affect to positive affect, then perhaps the data signal is too weak to draw conclusions for this variable. This is the case for Internet usage - darnit Covid19.</p><p>The posterior distributions can also be viewed as traceplots.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">avz</span><span class="p">.</span><span class="nf">plot_trace</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_21_0.png" alt="png" /></p><h2 id="interpret-variable-affect-on-predicted-grade">Interpret Variable affect on Predicted Grade</h2><p>With the sampling complete. We can explore how each feature affects and contributes to predicted grade. I found this <code class="language-plaintext highlighter-rouge">model_affect()</code> function in this <a href="https://gist.github.com/WillKoehrsen/fa59f7f28aefa09bc80138d3de8d6052#file-query_vars-py">Gist</a> for PyMC3.</p><p>Again, the number of absences has a negative affect on epected grade. Increasing age has a positive affect.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">glm_</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">glm_</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">param_names</span><span class="p">)</span> <span class="c1"># dmatrix
</span><span class="nf">model_affect</span><span class="p">(</span><span class="sh">'</span><span class="s">absences</span><span class="sh">'</span><span class="p">,</span><span class="n">trace</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
<span class="nf">model_affect</span><span class="p">(</span><span class="sh">'</span><span class="s">age</span><span class="sh">'</span><span class="p">,</span><span class="n">trace</span><span class="p">,</span><span class="n">X</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_24_0.png" alt="png" /> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_24_1.png" alt="png" /></p><h2 id="posterior-predictive-checks">Posterior Predictive Checks</h2><p><a href="https://docs.pymc.io/notebooks/posterior_predictive.html">Posterior predictive checks (PPC’s)</a> are conducted to validate that the Bayesian model has captured the true distribution of the underlying data. Alternatively, PPC’s are used to evaluate how the true data distribution compares to the distribution of data generates by the Bayesian model. . As per the PyMC3 documentation, PPC’s are also a crucial part of the Bayesian modeling workflow. PPC’s have two main benefits:</p><ul><li><em>They allow you to check whether you are indeed incorporating scientific knowledge into your model – in short, they help you check how credible your assumptions before seeing the data are.</em><li><em>They can help sampling considerably, especially for generalized linear models, where the outcome space and the parameter space diverge because of the link function.</em></ul><p>Below, PPC is used to sample the Y outcome of our model 200 times (left). Similar to the above trace plots, PPC can also provide a sampling of model parameters such as Age (right).</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_26_5.png" alt="png" /></p><h2 id="predictions-on-out-of-sample-data">Predictions on Out-Of-Sample data</h2><p>Now that the model is trained and fitted on the data and we have inspected variable affects we can use the model to make predictions on out-of-sample observations and test cases. In contrast to other modeling approaches and packages, such as statsmodels and scikit-learn, it is not as straight forward as simply calling <code class="language-plaintext highlighter-rouge">model.predict(Xdata)</code>. In PyMC3 I have discovered several strategies for applying models to out-of-sample data.</p><h3 id="method-1-the-mean-coefficient-model">method 1: the mean coefficient model</h3><p>We can use the MCMC trace to obtain a sample mean of each model coefficient and apply this to reconstruct a typical GLM formula. Remember we used statsmodels-patsy formulation to encode our categorical variables, well we can again use patsy to construct a helper.</p><p>The benefit to this approach is its ease and simplicity. The downside is that we are now omitting and missing out on a chunk of that MCMC sampling for the confidence and uncertainty in our data that we obtained by taking a Bayes approach in the first place.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="c1"># mean model coefficients
</span><span class="n">mean_model</span> <span class="o">=</span> <span class="n">summary</span><span class="p">[[</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">]].</span><span class="n">T</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># create a  design matrix of exog data values
# same as for GLM's
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">glm_</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">exog</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">glm_</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">param_names</span><span class="p">)</span>

<span class="c1"># add columns for the standard deviations output from the bayesian fit
</span><span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mean_model</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>
        <span class="n">X</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        
<span class="c1"># multiply and work out mu predictions
</span><span class="n">coefs</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">mean_model</span><span class="p">.</span><span class="n">values</span>
<span class="n">pred_mu</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)[:]</span>
<span class="n">pred_sd</span> <span class="o">=</span> <span class="n">coefs</span><span class="p">[</span><span class="sh">'</span><span class="s">sd</span><span class="sh">'</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Predictions:</span><span class="sh">'</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">m</span><span class="p">,</span><span class="n">s</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">pred_mu</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span><span class="n">pred_sd</span><span class="p">[:</span><span class="n">n</span><span class="p">]):</span> <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\t</span><span class="s">Mu:</span><span class="si">{</span><span class="n">m</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> Sd:</span><span class="si">{</span><span class="n">s</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>Predictions:
	Mu:13.47 Sd:2.29
	Mu:12.34 Sd:2.29
	Mu:11.41 Sd:2.29
	Mu:13.48 Sd:2.29
	Mu:12.72 Sd:2.29
</pre></table></code></div></div><blockquote><p><strong><em>NOTE:</em></strong>: There is an issue using Jupyter, pymc3.GLM and Theano. At the time of writing this is where things with <code class="language-plaintext highlighter-rouge">pm.GLM.from_formula()</code> start to break down using Jupyter. The following two methods are recommended in the PyMC3 documentation. However, both generate the following Theano error when used in conjunction with <code class="language-plaintext highlighter-rouge">GLM.from_formula()</code> on Jupyter. <code class="language-plaintext highlighter-rouge">ERROR! Session/line number was not unique in database. History logging moved to new session 11</code> This seems to be an issue with the way GLM.from_formula() uses patsy and interacts with Theano in Jupyter Notebooks.<br /> <a href="https://github.com/pymc-devs/pymc3/blob/master/pymc3/glm/linear.py#L101"><em>source code</em></a><br /> <a href="https://stackoverflow.com/questions/50369957/dependency-between-session-line-number-was-not-unique-in-database-error-and-p"><em>question on SO</em></a><br /> I have not tested running either of the following methods using a .py script but it seems reasonable that the following methods would work outside Jupyter.</p></blockquote><h3 id="method-2-using-theano-shared-variable">method 2: using Theano shared variable</h3><p>Given the above, for now we will lose <code class="language-plaintext highlighter-rouge">GLM.from_formula()</code> and reconstruct the model in standard PyMC3 form and using both: patsy to generate a design matrix, and Theano to create a shared X variable. To keep things short I have simplified my Betas using <code class="language-plaintext highlighter-rouge">shape(n)</code>. This will degrade model tuning performance as all priors are set at the uniform initial value and it may lead to some zero errors <a href="https://discourse.pymc.io/t/mass-matrix-contains-zeros-on-the-diagonal/4981">e.g see here</a>. In practice, you should set the Beta’s individually using informative priors.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">patsy</span>
<span class="kn">from</span> <span class="n">theano</span> <span class="kn">import</span> <span class="n">shared</span>

<span class="n">design</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="nf">dmatrices</span><span class="p">(</span><span class="n">formula_like</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span><span class="n">return_type</span><span class="o">=</span><span class="sh">'</span><span class="s">dataframe</span><span class="sh">'</span><span class="p">)</span>
<span class="c1"># design[1].design_info
</span>
<span class="n">train</span><span class="p">,</span><span class="n">test</span> <span class="o">=</span> <span class="n">df</span><span class="p">[:</span><span class="mi">500</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="mi">500</span><span class="p">:]</span>
<span class="n">trainx</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="nf">build_design_matrices</span><span class="p">([</span><span class="n">design</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">design_info</span><span class="p">],</span><span class="n">train</span><span class="p">,</span><span class="n">return_type</span><span class="o">=</span><span class="sh">"</span><span class="s">dataframe</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">testx</span> <span class="o">=</span> <span class="n">patsy</span><span class="p">.</span><span class="nf">build_design_matrices</span><span class="p">([</span><span class="n">design</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">design_info</span><span class="p">],</span><span class="n">test</span><span class="p">,</span><span class="n">return_type</span><span class="o">=</span><span class="sh">"</span><span class="s">dataframe</span><span class="sh">"</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Shared theano variable for modeling
# must be np.array()
</span><span class="n">modelx</span> <span class="o">=</span> <span class="nf">shared</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">trainx</span><span class="p">))</span>

<span class="n">bglm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">bglm</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="n">a</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">modelx</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">trainy</span><span class="p">)</span>  
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="n">init</span><span class="o">=</span><span class="sh">"</span><span class="s">adapt_diag</span><span class="sh">"</span><span class="p">,</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><div><style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; }</style><progress value="8000" class="" max="8000" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [8000/8000 00:20&lt;00:00 Sampling 4 chains, 0 divergences]</div><p><br /> Now we can update our shared X variable with the test set and use the model to make predictions. The prediction here really means sampling the posterior distributions of each coefficient on the test set observations.</p><p>We can specify the number of sampling rounds to perform and visualise individual samples and aggregates of samples.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="n">samples</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># Update model X and make Prediction
</span><span class="n">modelx</span><span class="p">.</span><span class="nf">set_value</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">testx</span><span class="p">))</span> <span class="c1"># update X data
</span><span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">bglm</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="n">samples</span><span class="p">,</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">ppc</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Observed &amp; Predicted Grades</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">test</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">()[</span><span class="sh">'</span><span class="s">G3</span><span class="sh">'</span><span class="p">],</span><span class="sh">'</span><span class="s">k-</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Y observed</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">ppc</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">,:],</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Y 1st trace</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">ppc</span><span class="p">[</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">][:</span><span class="n">n</span><span class="p">,:].</span><span class="nf">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">grey</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">Y trace [0:</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s">]th mean</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
</pre></table></code></div></div><div><style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; }</style><progress value="50" class="" max="50" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [50/50 00:00&lt;00:00]</div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_34_3.png" alt="png" /></p><h3 id="method-3-shared-x-variable">method 3: shared X variable</h3><p>This method is very similar above but instead using the <code class="language-plaintext highlighter-rouge">pm.Data()</code> to hold our X data in train and test rounds. Functionally this is cleaner as we don’t need to import and use Theano <code class="language-plaintext highlighter-rouge">shared()</code> explicitly.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="n">bglm</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">bglm</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">alpha</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">betas</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">beta</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">HalfNormal</span><span class="p">(</span><span class="sh">'</span><span class="s">sigma</span><span class="sh">'</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">xdata</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Data</span><span class="p">(</span><span class="sh">"</span><span class="s">pred</span><span class="sh">"</span><span class="p">,</span> <span class="n">trainx</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pm</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">xdata</span><span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nc">Normal</span><span class="p">(</span><span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">trainy</span><span class="p">)</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span><span class="n">init</span><span class="o">=</span><span class="sh">"</span><span class="s">adapt_diag</span><span class="sh">"</span><span class="p">,</span><span class="n">return_inferencedata</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Update X values and predict outcomes and probabilities
</span><span class="k">with</span> <span class="n">bglm</span><span class="p">:</span>
    <span class="n">pm</span><span class="p">.</span><span class="nf">set_data</span><span class="p">({</span><span class="sh">"</span><span class="s">pred</span><span class="sh">"</span><span class="p">:</span> <span class="n">testx</span><span class="p">.</span><span class="n">T</span><span class="p">})</span>
    <span class="n">posterior_predictive</span> <span class="o">=</span> <span class="n">pm</span><span class="p">.</span><span class="nf">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">],</span> <span class="n">samples</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>
    <span class="n">model_preds</span> <span class="o">=</span> <span class="n">posterior_predictive</span><span class="p">[</span><span class="sh">"</span><span class="s">y</span><span class="sh">"</span><span class="p">]</span>
</pre></table></code></div></div><div><style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; }</style><progress value="8000" class="" max="8000" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [8000/8000 00:20&lt;00:00 Sampling 4 chains, 0 divergences]</div><div><style> /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; }</style><progress value="600" class="" max="600" style="width:300px; height:20px; vertical-align: middle;"></progress> 100.00% [600/600 00:02&lt;00:00]</div><p><br /> Again, let’s increase the number of samples visualise the:</p><ul><li>posterior distribution of predictions for each observation in the OOS test data (left)<li>and the Observed, Mean, Credible-Interval or “Highest Density Interval” using arviz <code class="language-plaintext highlighter-rouge">hdi()</code>.</ul><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">matplotlib.gridspec</span> <span class="kn">import</span> <span class="n">GridSpec</span>

<span class="n">fig</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">gs</span><span class="o">=</span><span class="nc">GridSpec</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span><span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span> <span class="c1"># 2 rows, 3 columns
</span>
<span class="n">ax0</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax0</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Yi kde</span><span class="sh">"</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">kdeplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">model_preds</span><span class="p">[:,:</span><span class="mi">5</span><span class="p">]),</span><span class="n">ax</span><span class="o">=</span><span class="n">ax0</span><span class="p">)</span>

<span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">test predictions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">test</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="sh">'</span><span class="s">G3</span><span class="sh">'</span><span class="p">],</span><span class="sh">'</span><span class="s">k-</span><span class="sh">'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">obs</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">model_preds</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">orange</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">mean pred</span><span class="sh">'</span><span class="p">)</span>

<span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">-</span><span class="mf">0.5</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">avz</span><span class="p">.</span><span class="nf">hdi</span><span class="p">(</span><span class="n">model_preds</span><span class="p">,</span><span class="n">alpha</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">ls</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">CI lower </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
<span class="n">ax1</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">avz</span><span class="p">.</span><span class="nf">hdi</span><span class="p">(</span><span class="n">model_preds</span><span class="p">,</span><span class="n">alpha</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">ls</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">'</span><span class="s">CI upper </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

<span class="n">ax1</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">));</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/images/2021-02-20-pymc3-predictions/output_38_1.png" alt="png" /></p><h1 id="conclusion">Conclusion</h1><p>This post demonstrates how to develop a Bayesian inference General Linear Model. A case study for modeling student grades was used to demonstrate a classical frequentist approach in statsmodels and with a Bayes’s approach in PyMC3 with several implementations on predicting out of sample data.</p><h1 id="references">References</h1><hr /><ul><li><a href="https://docs.pymc.io/notebooks/getting_started.html?highlight=glm">Getting started with GLM in PyMC3</a><li><a href="https://docs.pymc.io/notebooks/GLM.html">All GLM examples in PyMC3</a><li><a href="https://docs.pymc.io/notebooks/GLM-robust.html">Robust GLM’s in PyMC3</a><li><a href="https://docs.pymc.io/notebooks/GLM-linear.html">PyMC3 OLS Regression</a><li><a href="https://docs.pymc.io/notebooks/GLM-logistic.html">PyMC3 Logistic Regression</a><li><a href="https://stackoverflow.com/questions/37312817/pymc3-bayesian-linear-regression-prediction-with-sklearn-datasets">PyMC3 Bayesian Linear Regression prediction with sklearn.datasets</a><li><a href="https://gist.github.com/WillKoehrsen/fa59f7f28aefa09bc80138d3de8d6052#file-query_vars-py">Bayes affect plot</a></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/blog-post/'>blog-post</a>, <a href='/categories/data-analysis/'>data-analysis</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/bayesian-inference/" class="post-tag no-text-decoration" >bayesian-inference</a> <a href="/tags/bayes-theory/" class="post-tag no-text-decoration" >bayes-theory</a> <a href="/tags/pymc3/" class="post-tag no-text-decoration" >pymc3</a> <a href="/tags/glm/" class="post-tag no-text-decoration" >glm</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Bayesian Inference with PyMC3: pt 2 making predictions - Ben Postance&url=https://bpostance.github.io/posts/pymc3-predictions/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Bayesian Inference with PyMC3: pt 2 making predictions - Ben Postance&u=https://bpostance.github.io/posts/pymc3-predictions/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Bayesian Inference with PyMC3: pt 2 making predictions - Ben Postance&url=https://bpostance.github.io/posts/pymc3-predictions/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-ai-enterprise/">How to build an Advanced Analytics function</a><li><a href="/posts/multi-tasking-in-python/">Multi-tasking in Python</a><li><a href="/posts/airflow-taskflow/">Writing Pythonic Airflow DAGs with the TaskFlow API</a><li><a href="/posts/finCEN/">The finCEN files: Uncovering money laundering patterns in the global banking network</a><li><a href="/posts/working-with-MODIS-data/">Geospatial Analysis: Working with MODIS data</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/bayes-theory/">bayes-theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian-inference</a> <a class="post-tag" href="/tags/data-mining/">data-mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data-cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension-reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial-analysis</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/bayesian-inference-pymc3/"><div class="card-body"> <span class="timeago small" > Jan 15, 2021 <i class="unloaded">2021-01-15T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bayesian Inference with PyMC3: pt 1 posterior distributions</h3><div class="text-muted small"><p> Jupyter notebook here Introduction Here we use PyMC3 on two Bayesian inference case studies: coin-toss and Insurance Claim occurrence. My last post was an introduction to Baye’s theorem and Ba...</p></div></div></a></div><div class="card"> <a href="/posts/introduction-to-bayesian-inference/"><div class="card-body"> <span class="timeago small" > Dec 13, 2020 <i class="unloaded">2020-12-13T18:00:00+00:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Bayesian Inference by hand</h3><div class="text-muted small"><p> image source Jupyter notebook here Bayesian inference is a statistical method used to update one’s beliefs about a process or system upon observing data. It has wide reaching applications from...</p></div></div></a></div><div class="card"> <a href="/posts/glm-deep-dive/"><div class="card-body"> <span class="timeago small" > May 17, 2020 <i class="unloaded">2020-05-17T19:00:00+01:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>A deep dive on GLM's in frequency severity models</h3><div class="text-muted small"><p> Jupyter notebook here This notebook is a deep dive into General Linear Models (GLM’s) with a focus on the GLM’s used in insurance risk modeling and pricing (Yan, J. 2010).I have used GLM’s befor...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/bayesian-inference-pymc3/" class="btn btn-outline-primary" prompt="Older"><p>Bayesian Inference with PyMC3: pt 1 posterior distributions</p></a> <a href="/posts/clustering-mixed-data/" class="btn btn-outline-primary" prompt="Newer"><p>A guide to clustering large datasets with mixed data-types [updated]</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="text-center text-muted small pb-5"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> const options = { scriptUrl: '//https-bpostance-github-io.disqus.com/embed.js', disqusConfig: function() { this.page.title = 'Bayesian Inference with PyMC3: pt 2 making predictions'; this.page.url = 'https://bpostance.github.io/posts/pymc3-predictions/'; this.page.identifier = '/posts/pymc3-predictions/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('.post-content img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://twitter.com/username">BenPostance</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author."> Some rights reserved. </span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/bayes-theory/">bayes theory</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/data-mining/">data mining</a> <a class="post-tag" href="/tags/monte-carlo/">monte carlo</a> <a class="post-tag" href="/tags/classification/">classification</a> <a class="post-tag" href="/tags/clustering/">clustering</a> <a class="post-tag" href="/tags/data-cleaning/">data cleaning</a> <a class="post-tag" href="/tags/decomposition/">decomposition</a> <a class="post-tag" href="/tags/dimension-reduction/">dimension reduction</a> <a class="post-tag" href="/tags/geospatial-analysis/">geospatial analysis</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#top" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://bpostance.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script>
